## Creating Key Vault

 What is Azure Key Vault?
    â€¢ A secure service to store:
        â—‹ Credentials, passwords, database connection strings
        â—‹ PKI certificates
        â—‹ Secrets used by apps and services
    â€¢ Supports centralized secret management

ğŸ’¡ Key Vault Use Case
    â€¢ Encrypt Azure VM disks using customer-managed keys
    â€¢ ğŸ”‘ Store encryption keys in the same Azure region as the VM

ğŸ› ï¸ Creating a Key Vault (Portal)
    1. Go to Create a resource
    2. Search for: Key Vault
    3. Select Key Vault by Microsoft â†’ Click Create
    4. Choose:
        â—‹ Resource Group (e.g., App1)
        â—‹ Key Vault Name (e.g., KVEast1)
        â—‹ Region (e.g., East US)
            Â§ Must match the region of associated services like VMs

ğŸŒ Common Azure Regions
    â€¢ East US 2
    â€¢ Germany West Central
    â€¢ France Central
    â€¢ Japan East/West
    â€¢ Korea Central/South

ğŸ’² Pricing Tier
    â€¢ Standard: No HSM support
    â€¢ Premium: Includes Hardware Security Module (HSM)
        â—‹ Required for tamper-resistant key storage
        â—‹ Supports cryptographic operations

ğŸ§¹ Soft Delete & Retention Settings
    â€¢ Soft delete is enabled by default: 90-day recovery window
    â€¢ You can:
        â—‹ Enable purge protection (cannot delete during retention)
        â—‹ Disable purge protection (can delete within retention)
    â€¢ After setting, click Review + Create and Create

ğŸ§‘â€ğŸ’» Post-Creation: Access Configuration
    â€¢ Click Go to Resource to open the vault
    â€¢ Under Access Policies:
        â—‹ Assign access to users/services (e.g., IT Demo 5)
        â—‹ Click + Create to define policies

ğŸ“‹ Access Policy Templates
    â€¢ Key, Secret & Certificate Management
    â€¢ Secret Management only
    â€¢ SQL Server Connector
    â€¢ Azure Storage / Data Lake
    â€¢ Exchange/SharePoint Customer Keys
    â€¢ Azure Info Protection BYOK
    â€¢ M365 Data at Rest Encryption

ğŸ§‘ Assigning Users/Apps
    â€¢ Choose a user (e.g., Abu Adachi)
    â€¢ Optionally assign an app to access the vault
    â€¢ Click Next â†’ Create

ğŸ”’ Role-Based Access Control (IAM)
    â€¢ Navigate: Access Control (IAM) â†’ + Add role assignment
    â€¢ Example roles:
        â—‹ Key Vault Administrator: Full control
        â—‹ Key Vault Contributor: Manage vault but not contents
        â—‹ Key Vault Reader: Read-only access to settings
        â—‹ Other roles:
            Â§ Key Vault Certificates Officer
            Â§ Key Vault Crypto Officer
            Â§ Key Vault Crypto Service Encryption User
            Â§ Key Vault Crypto User

ğŸ§± Vault Object Management
    â€¢ Go to Properties > Objects
    â€¢ Manage:
        â—‹ ğŸ”‘ Keys
        â—‹ ğŸ” Secrets
        â—‹ ğŸ“œ Certificates (PKI)

ğŸ’» Command Line Management
Azure CLI
az keyvault create --help
    â€¢ az keyvault create: Create a new key vault
    â€¢ Use flags like --name, --resource-group, --location, --sku
PowerShell
Get-Command *keyvault*
    â€¢ Lists all key vault-related cmdlets
    â€¢ Common: New-AzKeyVault, Set-AzKeyVaultAccessPolicy


## Managing Key Vault Secrets Using the GUI



ğŸ—‚ï¸ Accessing Existing Key Vault
    â€¢ Go to All Resources in Azure portal.
    â€¢ Filter by Type: Key vault â†’ Click Apply.
    â€¢ Only appears if at least one vault exists (e.g., KVEast1 in East US).

ğŸŒ Region Awareness
    â€¢ Resources needing the vault (e.g., VMs) should be in the same region.
    â€¢ Use multiple vaults for:
        â—‹ Regional separation
        â—‹ Isolated IT team control

ğŸ” Access Requirements
    â€¢ Required to manage secrets:
        â—‹ IAM Role Assignments (via Access Control)
        â—‹ Access Policies (fine-grained permissions)
    â€¢ Policies may include:
        â—‹ âœ… Key Permissions
        â—‹ âŒ Secret/Certificate Permissions (may be absent)
    â€¢ Applies to users or services (VMs, storage, etc.)

ğŸ”‘ Managing Keys
    â€¢ Click Generate/Import:
        â—‹ Import: Use existing public/private key pair or backup.
        â—‹ Generate:
            Â§ Choose type: RSA or EC (Elliptic Curve)
            Â§ RSA sizes: 2048 (default), 3072, 4096
            Â§ EC uses smaller key sizes for equivalent strength
    â€¢ Optional:
        â—‹ Activation Date
        â—‹ Expiration Date
        â—‹ Time Zone (e.g., (UTC-04:00) Atlantic Time)
    â€¢ Default: Key is Enabled
    â€¢ Consider setting rotation policy for security compliance.

ğŸ”’ Key Details
    â€¢ Select key (e.g., RSAKeyPair1) â†’ Click Current Version
        â—‹ âœ… Download Public Key
        â—‹ âŒ Cannot download Private Key
            Â§ Private key remains securely in the vault
            Â§ Used for:
                â–¡ ğŸ” Decryption (when encrypted with the public key)
                â–¡ âœï¸ Digital Signature Creation
            Â§ Public key used to verify signatures
    â€¢ Under "Permitted operations" â†’ See options like Sign

ğŸ§¬ Managing Secrets
    â€¢ Click Generate/Import â†’ Choose Manual
        â—‹ Certificate option is deprecated
    â€¢ Use case:
        â—‹ Tokens, passwords, access keys, connection strings
    â€¢ Example:
        â—‹ Name: SecretValue1
        â—‹ Value: (entered manually)
    â€¢ Notes:
        â—‹ Must be single-line in GUI
        â—‹ Multi-line secrets â†’ Use CLI or API
    â€¢ Same options as keys:
        â—‹ Activation/Expiration
        â—‹ Enable/Disable

âœ… Secret is now saved (e.g., SecretValue1)
ğŸ“Œ PKI Certificates handled in separate demo

## Managing Key Vault Secrets Using the CLI

AZ-500 Study Notes: Managing Key Vault Secrets Using the CLI

ğŸ“¥ Environment Setup
    â€¢ Open Cloud Shell in the Azure portal
    â€¢ Switch to Bash for Linux-style variable handling (optional)
        â—‹ PowerShell also supports Azure CLI

ğŸ—ï¸ Creating a Key Vault via CLI
az keyvault create \
  --location eastus \
  --name KVEast2 \
  --resource-group App1 \
  --network-acls-ips <public_IP>
    â€¢ --location: Azure region for the vault
    â€¢ --name: Name of the Key Vault
    â€¢ --resource-group: Target RG
    â€¢ --network-acls-ips: IP(s) allowed to access (e.g., on-prem public IP)
    
âœ… Vault is now accessible from defined IPs
ğŸ§­ Confirm via portal under All Resources â†’ Filter Key vault

ğŸ“‹ List Key Vaults

az keyvault list --query [].name

    â€¢ Lists only names of all key vaults in the subscription

ğŸ” Create a Secret via CLI
az keyvault secret set \
  --name db1connection \
  --value "connectionstringsamplevalue" \
  --vault-name KVEast2
    â€¢ --name: Secret name
    â€¢ --value: Secret value
    â€¢ --vault-name: Target vault
    
ğŸ“Œ Use for:
    â€¢ Access tokens
    â€¢ DB connection strings
    â€¢ Secure app config values

ğŸ” List Secrets in Vault
az keyvault secret list --vault-name KVEast2
    â€¢ Lists all secrets with full metadata
Optional filter:
az keyvault secret list --vault-name KVEast2 --query [].name
    â€¢ Returns only names

ğŸ”‘ Create a Key Pair via CLI
az keyvault key create \
  --name RSAKeyPair2 \
  --kty RSA \
  --protection software \
  --vault-name KVEast2
    â€¢ --name: Key name
    â€¢ --kty: Key type (e.g., RSA, EC, EC-HSM)
    â€¢ --protection: Storage type (software or HSM)
    â€¢ --vault-name: Vault to store the key

ğŸ” List Keys in Vault
az keyvault key list --vault-name KVEast2 --query [].name

    â€¢ Lists all key names in the specified vault
    

ğŸ” Key Details in Portal
    â€¢ Open KVEast2 â†’ Keys â†’ Select RSAKeyPair2 â†’ CURRENT VERSION
        â—‹ View:
            Â§ âœ… Permitted operations: Encrypt, Decrypt, Sign, Verify
            Â§ âœ… Download public key
            Â§ âŒ Private key remains vault-only

## Managing Key Vault Secrets Using PowerShell

AZ-500 Study Notes: Managing Key Vault Secrets Using PowerShell

âš™ï¸ Creating a Key Vault with PowerShell
New-AzKeyVault `
  -Name "KVEast3" `
  -ResourceGroupName "App1" `
  -Location "East US" `
  -EnabledForDeployment
    â€¢ -EnabledForDeployment: Allows Azure resources like VMs to access the vault (e.g., for disk encryption)

ğŸ“› Common PowerShell Cmdlets
Get-Command *keyvault*
    â€¢ Lists all available Key Vault cmdlets:
        â—‹ New-AzKeyVault
        â—‹ Set-AzKeyVaultAccessPolicy
        â—‹ Set-AzKeyVaultSecret
        â—‹ Add-AzKeyVaultKey
        â—‹ Get-AzKeyVaultKey, etc.

ğŸ” Set Access Policy
Set-AzKeyVaultAccessPolicy `
  -VaultName "KVEast3" `
  -UserPrincipalName "cblackwell@quick24x7testing.onmicrosoft.com" `
  -PermissionsToSecrets all `
  -PermissionsToKeys all `
  -PermissionsToCertificates all `
  -PermissionsToStorage get
    â€¢ Grants Codey Blackwell full access to secrets, keys, certs
    â€¢ Storage permission (get) allows pulling from storage-linked vault

ğŸ“‹ List Key Vaults
Get-AzKeyVault
    â€¢ Lists all vaults in the subscription
    â€¢ Use Select-Object Name to limit output

ğŸ” Create a Secure Secret
$secretvalue = ConvertTo-SecureString "MySecurePass!" -AsPlainText -Force
    â€¢ Converts string to secure format in memory
$secret = Set-AzKeyVaultSecret `
  -VaultName "KVEast3" `
  -Name "password1" `
  -SecretValue $secretvalue
    â€¢ Stores secret in Key Vault named password1

ğŸ› ï¸ Fix Access Denied Issues
    â€¢ If Generate/Import is grayed out or errors occur:
        â—‹ Use Access policies in the portal
        â—‹ Add current user with full permissions to secrets, keys, certs

ğŸ”‘ Create a Key Pair
Add-AzKeyVaultKey `
  -VaultName "KVEast3" `
  -Name "RSAKeyPair4" `
  -Destination "Software"
    â€¢ -Destination: "Software" for soft keys, "HSM" for hardware-backed
    â€¢ Optional:
        â—‹ -KeyType EC â†’ Elliptic Curve

ğŸ” List Keys in Vault
Get-AzKeyVaultKey -VaultName "KVEast3"
Filter to show only names:
Get-AzKeyVaultKey -VaultName "KVEast3" | Select-Object Name

## Managing Key Vault Certificates Using the GUI

Overview
    â€¢ Azure Key Vault can store:
        â—‹ Passwords
        â—‹ Key pairs (public/private)
        â—‹ PKI certificates (e.g., for HTTPS on custom domains)

Accessing the Key Vault
    1. Go to All Resources
    2. Filter Type to show only Key vault
    3. Open vault (e.g., KVEast3)

Certificate Creation Options
Click Certificates â†’ Generate/Import
Method of Certificate Creation:
    â€¢ Generate: Create a new certificate within Azure
    â€¢ Import: Upload an existing certificate (e.g., PFX)

Certificate Authority (CA) Types
    1. Self-signed certificate
        â—‹ Signed by the vault itself
        â—‹ Not trusted by browsers unless root CA is installed
    2. Certificate issued by integrated CA
        â—‹ Requires setting up with providers like DigiCert or GlobalSign
        â—‹ Must provide account credentials (Account ID, Password, Org ID)
    3. Certificate issued by non-integrated CA
        â—‹ Generates a Certificate Signing Request (CSR)
        â—‹ Submit CSR externally to a CA (manual trust chain)
        â—‹ Not managed by KV

Example: Creating a Self-Signed Certificate
    â€¢ Name: Self-signed Certificate1
    â€¢ Common Name (CN): www.webapp1test.com
    â€¢ Add DNS name (must match CN for browser trust)
    â€¢ Validity: 12 months
    â€¢ Auto-Renewal:
        â—‹ Option: Renew at 80% lifetime
        â—‹ Alternative: Set reminder emails
    â€¢ Public/private key pair is auto-generated

Advanced Policy Configuration
    â€¢ Accessed via Not configured link during creation
    â€¢ Set:
        â—‹ Key Usage Flags (e.g., Digital Signature, Key Encipherment)
        â—‹ Key type: RSA or EC (Elliptic Curve)
        â—‹ Key size: 2048, 3072, 4096
        â—‹ Exportable private key: Enabled or Disabled
        â—‹ Certificate Transparency: Enabled by default

After Creation
    â€¢ Certificate appears immediately
    â€¢ Initial status: Disabled â†’ auto-transitions to Enabled
    â€¢ Can now be used by:
        â—‹ Web apps (App Services)
        â—‹ VMs (using managed identity)

Permissions Reminder
    â€¢ The service (e.g., web app) must have access to Key Vault
        â—‹ Either through:
            Â§ Access Policies
            Â§ Role-Based Access Control (RBAC) via Access Configuration

## Managing Key Vault Certificates Using the CLI

AZ-500 Study Notes: Managing Key Vault Certificates Using the CLI

Viewing Key Vaults in CLI
az keyvault list
    â€¢ Lists all Key Vaults in the subscription
az keyvault list --query [].name
    â€¢ Filters output to only show Key Vault names

List Certificates in a Vault
az keyvault certificate list --vault-name KVEast3
    â€¢ Lists all certificate objects in the specified vault
az keyvault certificate list --vault-name KVEast3 --query [].name
    â€¢ Filters to show only certificate names

Create a New Certificate
az keyvault certificate create \
  --vault-name KVEast3 \
  --name Cert3 \
  --policy "$(az keyvault certificate get-default-policy)"
Parameters:
    â€¢ --vault-name: Name of the Key Vault to create the cert in
    â€¢ --name or -n: Name of the new certificate
    â€¢ --policy or -p: Certificate policy (default policy used here)
az keyvault certificate get-default-policy:
    â€¢ Returns a JSON object with the default certificate policy
    â€¢ Includes key usage, validity, export settings, etc.

Post-Creation Confirmation
    â€¢ Refresh the Certificates section in the portal under KVEast3
    â€¢ Certificate Cert3 will appear and should be in Enabled status

Certificate Details (via Portal)
    â€¢ Click on certificate (e.g., Cert3) â†’ Current Version
    â€¢ Options available:
        â—‹ Download as:
            Â§ .CER (public only)
            Â§ .PEM / .PFX (includes private key, requires password)
    â€¢ View metadata:
        â—‹ Activation Date: e.g., 03/22/2023 11:47:47 AM
        â—‹ Expiration Date: e.g., 03/22/2024 11:57:47 AM
        â—‹ Region: UTC-04:00 (Atlantic Time)

Summary
    â€¢ CLI enables full lifecycle management of Key Vault certificates
    â€¢ Use the default policy unless a custom policy is needed
    â€¢ Private key downloads require password protection
    â€¢ Certificate uses include encryption, signing, and HTTPS support for apps

##  Managing Key Vault Certificates Using PowerShell

AZ-500 Study Notes: Managing Key Vault Certificates Using PowerShell

List Key Vaults
Get-AzKeyVault
    â€¢ Lists all Key Vaults in the subscription
Filter to only show vault names:
Get-AzKeyVault | Select-Object VaultName
Determine available object properties:
Get-AzKeyVault | Get-Member -MemberType Property
    â€¢ Confirms property names like VaultName (no space)

Define Certificate Policy
$Policy = New-AzKeyVaultCertificatePolicy `
  -SecretContentType 'application/x-pkcs12' `
  -SubjectName "CN=www.app1testing.com" `
  -IssuerName "Self" `
  -ValidityInMonths 12 `
  -ReuseKeyOnRenewal
Parameter Explanation:
    â€¢ -SecretContentType: Certificate format (PKCS12)
    â€¢ -SubjectName: CN (Common Name) used in the cert
    â€¢ -IssuerName: "Self" for self-signed
    â€¢ -ValidityInMonths: How long the cert is valid (e.g., 12 = 1 year)
    â€¢ -ReuseKeyOnRenewal: Reuses the same key instead of generating new one on renewal

Create a Certificate
Add-AzKeyVaultCertificate `
  -VaultName "KVEast3" `
  -Name "App2Cert" `
  -CertificatePolicy $Policy
    â€¢ Creates a certificate in vault KVEast3 named App2Cert using the $Policy defined above

Confirm Certificate Creation
Get-AzKeyVaultCertificate -VaultName "KVEast3" | Select-Object Name
    â€¢ Lists all certificates in the vault by name

Portal Confirmation
    â€¢ Go to KVEast3 â†’ Certificates â†’ Click Refresh
    â€¢ New cert (App2Cert) will first show as Disabled
    â€¢ After a moment, status updates to Completed

Notes
    â€¢ Self-signed certificates are not trusted by default in browsers or services unless the root CA is installed manually.
    â€¢ Certificates can be used by:
        â—‹ Azure App Services
        â—‹ Custom HTTPS domains
        â—‹ Digital signing / encryption


## Working with Azure Key Vault and Hardware Security Modules (HSMs)

What is a Hardware Security Module (HSM)?
    â€¢ A dedicated, tamper-resistant hardware appliance
    â€¢ Performs cryptographic operations:
        â—‹ Key generation
        â—‹ Encryption/Decryption
        â—‹ Digital signatures
        â—‹ Authentication
    â€¢ FIPS 140-2 Level 3 compliant
    â€¢ Often required for regulatory compliance:
        â—‹ PCI DSS
        â—‹ GDPR
        â—‹ HIPAA

# Managing Az Policies

What Is Azure Policy?
    â€¢ A governance tool to enforce rules and effects on Azure resources.
    â€¢ Controls what can be deployed, how it's configured, and where it can be deployed.

Azure Policy vs RBAC
    â€¢ RBAC = Controls who can perform what actions.
    â€¢ Azure Policy = Controls what resources can be created/configured and how.
Example:
    â€¢ RBAC: "Techs can deploy VMs."
    â€¢ Policy: "Only Linux VMs of size B2ms can be deployed in West US."

Built-in and Custom Policies
    â€¢ Built-in policies: Ready-to-use for common compliance needs.
    â€¢ Custom policies: Created using a JSON definition file.

Policy Structure
    â€¢ JSON-based
    â€¢ Can use parameters (e.g., allowed locations, VM sizes)
    â€¢ Example parameter: "allowedLocations": ["westus", "eastus"]

Scope of Assignment
    â€¢ Assign policies to levels of Azure hierarchy:
        â—‹ Management Group â†’ affects all subscriptions beneath it
        â—‹ Subscription
        â—‹ Resource Group
    â€¢ You can exclude specific resource groups/projects even if they're under a broader scope

Policy Effects
Effect	Description
Append	Adds settings to a resource during deployment (e.g., storage account rules)
Audit	Logs non-compliance in Activity Log
AuditIfNotExists	Logs if a related configuration is missing (e.g., encryption not enabled)
DeployIfNotExists	Checks and deploys a resource/config if not already present
Deny	Blocks resource creation if it violates policy (e.g., wrong region)


Tag Governance Example
    â€¢ Policy can add default tags if none are specified
    â€¢ Ensures consistent tagging across environment

Policy Initiatives
    â€¢ A group of policies assigned together
    â€¢ Assigned like a single policy, but includes many under the hood
    â€¢ Example initiative:
        â—‹ Policy 1: Enforce allowed regions
        â—‹ Policy 2: Require VM disk encryption
        â—‹ Policy 3: Require endpoint protection
    â€¢ Benefit: Simplifies management and reporting

Management Tools
    â€¢ Azure Policy can be managed through:
        â—‹ Azure Portal
        â—‹ Azure CLI
        â—‹ PowerShell

## Using Azure Policy to Audit Compliance

Purpose of Azure Policy Assignments
    â€¢ Enforce rules across management groups, subscriptions, or resource groups
    â€¢ Examples:
        â—‹ Ensure TDE is enabled for SQL
        â—‹ Enforce allowed regions for resource deployment
        â—‹ Check if disaster recovery is configured for VMs

Starting in the Portal
    1. In Azure portal, search â€œPolicyâ€
    2. Opens Azure Policy blade with:
        â—‹ Overview
        â—‹ Assignments
        â—‹ Definitions
        â—‹ Compliance

Definitions Tab
    â€¢ View policy definitions and initiative definitions
    â€¢ Filter by:
        â—‹ Definition Type: Policy or Initiative
        â—‹ Initiatives are groups of related policies whether its built in or custom created
        â—‹  Policy are individual
        â—‹ Search (e.g., "encrypt", "location")

Example: Allowed Locations Policy
    1. Filter or search for "allowed locations"
    2. View the JSON policy definition
        â—‹ parameters: listOfAllowedLocations (string array)
        â—‹ effect: deny
        â—‹ Condition: If resource location is not in allowed list, deny deployment

Assign the Policy
Click Assign â†’ Steps:
1. Basics (Scope and Exclusions)
    â€¢ Choose Scope:
        â—‹ Management Group
        â—‹ Subscription
        â—‹ Resource Group (e.g., App1)
    â€¢ Optional: Add Exclusions (e.g., exclude a project or resource)
2. Assignment Name
    â€¢ Example: Allowed locations for App1 resource group
3. Advanced (Optional)
    â€¢ Use Add Resource Selector to limit policy to specific resource types or locations
    â€¢ Skipped in this case (applies to everything in App1)
4. Parameters
    â€¢ Set the allowed region (e.g., East US)
    â€¢ Can select multiple regions if needed
5. Remediation (Optional)
    â€¢ Policy applies only to new resources
    â€¢ Existing resources require a remediation task
    â€¢ May need a managed identity for automatic remediation
6. Review + Create
    â€¢ Validate settings and click Create

Confirm Policy Assignment
    â€¢ Go to Assignments tab
    â€¢ Set Scope filter to App1 resource group
    â€¢ See assignment listed with name and compliance state

View Compliance State
    â€¢ Go to Compliance tab
    â€¢ Find policy assignment (e.g., Allowed Locations)
    â€¢ State will show as Compliant

Test Denial Enforcement
    1. Try to create a resource (e.g., Storage Account)
    2. Set:
        â—‹ Resource Group: App1
        â—‹ Region: Central US (not allowed)
    3. Azure blocks creation:
        â—‹ Error: â€œPolicy validation failedâ€
        â—‹ Message: Region is not in allowedLocations

Summary
    â€¢ Azure Policy lets you enforce, audit, or auto-remediate compliance
    â€¢ Supports built-in or custom JSON policies
    â€¢ Assign to any scope (MG, sub, RG)
    â€¢ Monitor enforcement under Assignments and Compliance

## Creating and Assigning a Custom Policy

Why Use a Custom Policy?
    â€¢ Built-in policies may not cover highly specific business or technical requirements.
    â€¢ Example: Only allow Ubuntu 20.04 VMs in a specific resource group.

Steps to Create a Custom Policy (Portal)
1. Open Azure Policy
    â€¢ Search for â€œPolicyâ€ in the Azure Portal
    â€¢ Go to Definitions tab
2. Create New Policy Definition
    â€¢ Click Add policy definition
    â€¢ Choose Definition location: e.g., Azure subscription 1
    â€¢ Name: Ubuntu Forever
    â€¢ Category: Create new â†’ Virtual Machines
3. Paste JSON Policy Definition
    â€¢ JSON includes:
        â—‹ "if": Checks if resource is VM, disk, or scale set
            Â§ Matches:
                â–¡ "publisher": "Canonical"
                â–¡ "offer": "UbuntuServer"
                â–¡ "sku": "20.04-LTS"
        â—‹ "then": effect = deny if condition is not met
    â€¢ Save policy

Assign the Custom Policy
1. Click Assign from the policy definition screen
    â€¢ Scope: Choose App1 resource group
    â€¢ No exclusions
    â€¢ Assignment Name: Ubuntu Forever
    â€¢ Status: Enabled
2. Click through:
    â€¢ No Advanced filters
    â€¢ No Parameters (this policy has none)
    â€¢ No Remediation tasks
    â€¢ Click Create

Test the Assignment
âœ… Allowed:
    â€¢ Deploy Ubuntu 20.04 VM in App1 RG â†’ Passes policy check
âŒ Denied:
    â€¢ Deploy Windows Server or Ubuntu 18.04 in App1 RG â†’ Fails with policy violation
    â€¢ Policy error message includes name: Ubuntu Forever
    â€¢ Clicking Policy Details shows JSON and assignment info
âœ… Not Affected:
    â€¢ Create any VM in App2 RG â†’ Passes (policy not assigned to App2)

Policy Lifecycle Notes
    â€¢ Custom policies show Type = Custom under Definitions
    â€¢ Can filter definitions by:
        â—‹ Search term: e.g., Ubuntu
        â—‹ Category: e.g., Virtual Machines

Deleting a Custom Policy
    â€¢ Must first remove all assignments
    â€¢ Otherwise deletion will fail

Summary
    â€¢ Custom policies let you define fine-grained, resource-specific controls
    â€¢ Assignment scoping ensures policy only applies where needed
    â€¢ Built-in and GitHub examples can help with writing custom policy JSON
    
     
## Assigning Az policy using cli

AZ-500 Study Notes: Assigning Azure Policy Using the Command Line

Overview
Azure Policy assignments can be created via:
    â€¢ PowerShell
    â€¢ Azure CLI
Useful for automation, scripting, and environments without GUI access.

PowerShell: Assigning a Policy
1. Set the Resource Group Variable
$rg = Get-AzResourceGroup -Name "App1"
2. Retrieve the Policy Definition
$definition = Get-AzPolicyDefinition | Where-Object {
  $_.Properties.DisplayName -eq "Audit virtual machines without disaster recovery configured"
}
3. Create the Policy Assignment
New-AzPolicyAssignment `
  -Name "VMs-DR Enabled" `
  -DisplayName "Check for VM Disaster Recovery" `
  -Scope $rg.ResourceId `
  -PolicyDefinition $definition
    â€¢ -Scope: Must be the resource ID (not just the name)
    â€¢ -PolicyDefinition: Uses the full object retrieved earlier

Confirm in Portal
    â€¢ Go to Azure Policy â†’ Assignments
    â€¢ Set Scope to the resource group (App1)
    â€¢ Confirm policy appears (e.g., Check for VM Disaster Recovery)

Azure CLI: Assigning a Policy
1. List All Policy Definitions by Display Name
az policy definition list --query [].displayName
2. Gather Required IDs
    â€¢ Go to the portal â†’ Find the Policy Definition ID
    â€¢ Go to the Resource Group â†’ Copy its Resource ID
3. Create the Assignment via CLI
az policy assignment create \
  --name "UbuntuAssignment1" \
  --policy "/subscriptions/<sub-id>/providers/Microsoft.Authorization/policyDefinitions/<policy-id>" \
  --scope "/subscriptions/<sub-id>/resourceGroups/App1"
Parameters:
    â€¢ --name: Unique name for the assignment
    â€¢ --policy: Full resource path to the policy definition
    â€¢ --scope: Full resource path to the assignment scope (e.g., RG)

Confirm CLI Assignment in Portal
    â€¢ Azure Policy â†’ Assignments
    â€¢ Set Scope to App1
    â€¢ Look for UbuntuAssignment1 listed

Notes
    â€¢ Both PowerShell and CLI require:
        â—‹ Policy Definition ID
        â—‹ Scope Resource ID
    â€¢ Portal is helpful for copying those values directly
    â€¢ Assignments created this way are active immediately

## Managing policy initiatives

What Is a Policy Initiative?
    â€¢ A grouping of related policy definitions
    â€¢ Assigned as a single unit for streamlined governance
    â€¢ Useful for applying multiple policies at once for:
        â—‹ Regulatory compliance (e.g., PCI-DSS, NIST SP 800-171)
        â—‹ Organizational standards (e.g., web app security settings)

Viewing Built-in Initiatives
    1. Go to Azure Policy
    2. Navigate to Definitions
    3. In Definition type dropdown â†’ Select Initiative
    4. Examples:
        â—‹ PCI v3.2.1:2018
        â—‹ NIST SP 800-171 Rev.2

Assigning a Built-in Policy Initiative (e.g., PCI DSS)
1. Select Initiative
    â€¢ Click on PCI v3.2.1:2018
    â€¢ Click Assign
2. Define Scope
    â€¢ Choose:
        â—‹ Subscription
        â—‹ Resource Group (e.g., App1)
    â€¢ Optional: Add Exclusions
3. Advanced Options (optional)
    â€¢ Narrow assignment to:
        â—‹ Specific resource types
        â—‹ Specific locations
4. Parameters
    â€¢ Provide required values for any parameters
    â€¢ Some built-in initiatives prompt for multiple inputs
5. Remediation (optional)
    â€¢ Enable remediation tasks to fix existing non-compliant resources
    â€¢ May require managed identity
6. Non-Compliance Messages (optional)
    â€¢ Add guidance to help admins fix non-compliant resources
7. Review and Create
    â€¢ Click Create
    â€¢ Confirm success in Assignments view (scope = App1)

Creating a Custom Initiative
1. Go to Definitions â†’ Click Add Initiative Definition
    â€¢ Scope: Choose Subscription or Management Group
    â€¢ Name: e.g., Web App Security
    â€¢ Category: Choose existing (e.g., Security Center) or create new
2. Add Policy Definitions
    â€¢ Click Add policy definitions
    â€¢ Filter (e.g., App Service)
    â€¢ Select relevant policies, such as:
        â—‹ Require HTTPS for App Services
        â—‹ Disable public access to App Services
        â—‹ Enforce TLS 1.2
        â—‹ Enable Defender for App Services
3. (Optional) Organize Policy Groups
    â€¢ Group related policies for easier visibility
4. Skip Parameters (if none needed)
    â€¢ Or define default values for parameterized policies
5. Create the Initiative

Assigning a Custom Initiative
    â€¢ Either click Assign from the success page
    â€¢ Or go to Assignments tab â†’ Click Assign
    â€¢ Scope: e.g., Subscription
    â€¢ Review and Create

Viewing and Validating Assignment
    â€¢ Go to Assignments
    â€¢ Scope: Filter to Subscription or RG (e.g., App1)
    â€¢ See initiative listed (e.g., Web App Security)
    â€¢ Use Compliance tab to check evaluation status (may take time)

Notes
    â€¢ Initiatives reduce management overhead
    â€¢ Required for implementing regulatory frameworks
    â€¢ Useful for applying organization-specific bundles of policies

# EnableResourceLocking

## Azure Resource Locks

AZ-500 Study Notes: Azure Resource Locks

What Are Azure Resource Locks?
    â€¢ A way to protect resources from accidental changes or deletion
    â€¢ Can be applied at three levels:
        â—‹ Subscription
        â—‹ Resource Group
        â—‹ Individual Resource
    â€¢ Override RBAC permissions: even users with Owner or Contributor cannot perform restricted actions if a lock is in place

Why Use Resource Locks?
    â€¢ To prevent accidental deletion or unwanted modification of critical infrastructure
    â€¢ Useful in production environments, compliance zones, or during audits
    â€¢ Helps enforce governance even if roles allow destructive changes

Lock Types
    1. ReadOnly
        â—‹ Users can read the resource
        â—‹ Users cannot update or delete
        â—‹ Similar to RBAC Reader, but more restrictive:
            Â§ On a storage account, prevents listing keys
            Â§ No configuration changes allowed
    2. CanNotDelete
        â—‹ Users can read and modify
        â—‹ Cannot delete the resource
        â—‹ Example: You can still write to or update a VM, but you cannot delete the VM

Scope and Inheritance
    â€¢ Locks follow the Azure resource hierarchy
        â—‹ Lock at subscription â†’ applies to all resource groups and resources under it
        â—‹ Lock at resource group â†’ applies to all resources within it
    â€¢ New resources inherit lock status from parent
    â€¢ Most restrictive lock takes precedence in case of multiple locks across the hierarchy

Control Plane vs. Data Plane
Plane	Description	Affected by Lock
Control Plane	Resource configuration and lifecycle (e.g., VM settings, storage account deletion)	âœ… Yes
Data Plane	Actual resource data operations (e.g., writing a blob, querying SQL, RDP to VM)	âŒ No
    â€¢ Locks apply only to the control plane
    â€¢ Example: Locking a storage account does not prevent blob deletion within it

Behavior Examples
    â€¢ A CanNotDelete lock on a storage account:
        â—‹ Prevents deletion of the account itself
        â—‹ Does not stop users from deleting blobs or files inside
    â€¢ A ReadOnly lock on a VM:
        â—‹ Prevents resizing, reconfiguration, or deletion
        â—‹ Still allows reading properties

Additional Details for AZ-500
    â€¢ Locks apply immediately and persist until explicitly removed
    â€¢ You must remove a lock to delete the locked resource
    â€¢ Can be managed via:
        â—‹ Azure portal
        â—‹ ARM templates
        â—‹ Azure Policy (indirectly, through enforcement models)
    â€¢ Locking is an example of governance enforcement, along with:
        â—‹ Azure Policy
        â—‹ RBAC
        â—‹ Blueprints

> 
## Managing Azure Resource Locks Using the Portal

AZ-500 Study Notes: Managing Azure Resource Locks Using the Portal

Purpose of Resource Locks
    â€¢ Prevent accidental modification or deletion of Azure resources.
    â€¢ Applied at Subscription, Resource Group, or Individual Resource levels.
    â€¢ Overrides user RBAC permissions (e.g., Contributor role) for delete or modify actions.
Lock Types
    â€¢ Read-only: Prevents changes to resource configuration. Still allows data access in some cases but restricts control plane actions.
    â€¢ Delete: Allows changes but prevents deletion of the resource.
Hierarchical Application
    â€¢ Locks can be applied at:
        â—‹ Management Group level (No lock option visible in portal)
        â—‹ Subscription level (Resource locks available under Settings)
        â—‹ Resource Group level (Locks appear as "Locks" in left menu)
        â—‹ Individual Resource level (e.g., Storage Account)
    â€¢ Locks inherit downwards:
        â—‹ A lock at Subscription level applies to all child Resource Groups and resources.
        â—‹ New resources created under a locked scope inherit the lock.
    â€¢ Most restrictive lock wins if multiple locks exist at different levels.
Portal Navigation and Actions
At Subscription Level
    1. Navigate to Subscriptions.
    2. Choose subscription (e.g., Azure subscription 1).
    3. Under Settings, click Resource locks.
    4. Click Add to create a new lock.
    5. Enter:
        â—‹ Lock name: (e.g., SubscriptionLock1)
        â—‹ Lock type: Read-only or Delete
        â—‹ Optional: Add a Note
    6. Click OK.
At Resource Group Level
    1. Navigate to Resource groups.
    2. Select a group (e.g., App1).
    3. Under Settings, click Locks.
    4. Existing locks are displayed (including inherited locks).
    5. Click Refresh to update the view.
At Resource Level
    1. Navigate to All resources.
    2. Select a resource (e.g., a Storage Account).
    3. Under Settings, click Locks.
    4. View inherited or local locks.
Effect of Read-only Lock Example
    â€¢ On a Storage Account:
        â—‹ Cannot access Access Keys (control-plane action blocked).
        â—‹ Cannot change Networking settings (e.g., disable public access).
        â—‹ Portal displays error: "Failed to save firewall and virtual network settings" due to locked scope.
Removing Locks
    â€¢ You must remove a lock at the level it was applied.
    â€¢ E.g., a lock created at the Subscription cannot be deleted from a Resource Group or Resource level view.
    â€¢ Navigate to Azure subscription 1 > Resource locks, then click Delete.
Key Notes
    â€¢ Locks affect control plane only (not data plane):
        â—‹ E.g., Cannot delete Storage Account but can delete blobs inside.
    â€¢ Ensure to refresh portal views after deleting locks.
    â€¢ Locking provides additional governance alongside RBAC and Policy.

Best Practices for AZ-500
    â€¢ Use locks in production or audit-sensitive environments.
    â€¢ Always document lock types and purposes (use Notes field).
    â€¢ Combine locks with Azure Policy and Blueprints for strong governance.
    â€¢ Understand lock inheritance and evaluate where to apply them for maximum effectiveness.

## Managing Azure Resource Locks Using the CLI


AZ-500 Study Notes: Managing Azure Resource Locks Using the CLI

ğŸ”¹ Purpose of Resource Locks via CLI
    â€¢ Prevents accidental modification or deletion of resources.
    â€¢ Enforced at the control plane level (not data plane).
    â€¢ Overrides RBAC roles like Contributor for delete/modify.
    â€¢ Not a true security mechanism â€” anyone with enough privilege can remove the lock.

ğŸ”¹ Lock Types
    â€¢ CanNotDelete: Prevents deletion, allows modification.
    â€¢ ReadOnly: Prevents both deletion and modification.

ğŸ”¹ Hierarchical Scope Levels (Same as portal)
    â€¢ az account lock â†’ Subscription
    â€¢ az group lock â†’ Resource Group
    â€¢ az resource lock â†’ Individual Resource
ğŸ’¡ Locks inherit downward; most restrictive lock applies when multiple exist.

ğŸ”¹ Creating Locks Using CLI
1. At Subscription Level
az account lock create --name "Cannot delete subscription" --lock-type CanNotDelete
    â€¢ Applies to all child groups and resources.
    â€¢ Confirm with:
az account lock list
    â€¢ In portal: Go to Subscription > Resource Locks to validate.
    â€¢ Must manually click Refresh to see changes.

2. At Resource Group Level
az group lock create --lock-type ReadOnly -n NoModify -g App1
    â€¢ Blocks modification at group scope.
    â€¢ Confirm with:
az account lock list
    â€¢ Can also use:
az group lock list -g App1

3. At Resource Level (e.g., Storage Account)
az resource lock create \
  --lock-type ReadOnly \
  --name NoModify \
  --resource eastyhz1 \
  --resource-type Microsoft.Storage/storageAccounts \
  --resource-group App1
    â€¢ Required: --resource, --resource-type, and --resource-group
    â€¢ Will show up in:
az account lock list

ğŸ”¹ Viewing Lock Details
Show one specific lock
az account lock show --name "Cannot delete subscription"

ğŸ”¹ Attempting a Delete (and Failure Scenario)
    â€¢ Try to delete a resource under a locked scope (e.g., Application Insights).
    â€¢ Portal will allow you to go through the delete dialog.
    â€¢ No visible error, but Notification bell will report:
âŒ â€œFailed â€“ Scope is lockedâ€

ğŸ”¹ Removing Locks Using CLI
    â€¢ Subscription:
az account lock delete --name "Cannot delete subscription"
    â€¢ Resource Group:
az group lock delete --name "NoModify" --resource-group App1
    â€¢ Individual Resource:
az resource lock delete \
  --name "NoModify" \
  --resource-group App1 \
  --resource-type Microsoft.Storage/storageAccounts \
  --resource eastyhz1
ğŸŸ¡ Portal may not reflect lock removal until Refresh is clicked.

ğŸ”¹ Important Notes for AZ-500
    â€¢ Resource locks:
        â—‹ Affect control plane only.
        â—‹ Do not block data operations (e.g., deleting blobs inside a locked Storage Account).
    â€¢ Lock behavior may lag in the portal â€” always validate with Refresh.
    â€¢ CLI is preferred for bulk/automated lock management.

## Managing Azure Resource Locks Using PowerShell


AZ-500 Study Notes: Managing Azure Resource Locks Using PowerShell

ğŸ”¹ Overview
    â€¢ Purpose: Prevent accidental modification or deletion of resources.
    â€¢ Not a true security mechanism:
        â—‹ Users with appropriate RBAC roles (e.g., Owner) can remove locks.
        â—‹ Locks apply to control plane, not data plane.
    â€¢ Best used as part of governance (not standalone security).

ğŸ”¹ Common PowerShell Cmdlets
Get-Command *lock* -Type Cmdlet
Returns core cmdlets:
    â€¢ Get-AzResourceLock
    â€¢ New-AzResourceLock
    â€¢ Set-AzResourceLock
    â€¢ Remove-AzResourceLock

ğŸ”¹ View Existing Locks
Get-AzResourceLock
    â€¢ Lists all locks in current context (subscription, resource group, etc.)
    â€¢ Narrow down scope:
Get-AzResourceLock -ResourceGroupName "App1"

ğŸ”¹ Create Lock (Resource Group Level)
New-AzResourceLock -LockName "NoModify" -LockLevel ReadOnly -ResourceGroupName "App1" -Force
    â€¢ LockLevel: ReadOnly or CanNotDelete
    â€¢ -Force: Suppresses confirmation prompt
ğŸ’¡ PowerShell will prompt without -Force â€” useful to suppress in scripts

ğŸ”¹ Validate Lock
Get-AzResourceLock
    â€¢ Confirms lock exists:
        â—‹ Name: NoModify
        â—‹ LockLevel: ReadOnly
        â—‹ Scope: Resource Group App1
ğŸŸ¡ Portal may lag â€” always click Refresh after applying a lock.

ğŸ”¹ Modify Existing Lock
Set-AzResourceLock
    â€¢ Example use:
        â—‹ Add/update lock notes
        â—‹ Change lock type from ReadOnly â†’ CanNotDelete

ğŸ”¹ Remove Lock
Remove-AzResourceLock -ResourceGroupName "App1" -Name "NoModify" -Force
    â€¢ -Force: Skips confirmation
âœ… Returns True on success

ğŸ”¹ Final Lock Check
Get-AzResourceLock
    â€¢ Confirms all locks are cleared.

ğŸ”¹ Portal View (Optional)
    â€¢ Resource group â†’ Locks â†’ "NoModify"
    â€¢ Lock visible after Refresh
    â€¢ Options:
        â—‹ Edit (change type or add notes)
        â—‹ Delete

ğŸ”¹ Summary Notes for AZ-500
    â€¢ Locks apply at:
        â—‹ Subscription
        â—‹ Resource Group
        â—‹ Resource level
    â€¢ Inheritance flows downward
    â€¢ Most restrictive lock wins (if conflicts exist)
    â€¢ Locks override RBAC only temporarily â€” can be removed by privileged users
    â€¢ Best practice: use with Azure Policy or Blueprints for full governance


## Enabling Resource Locking with Templates


AZ-500 Study Notes: Enabling Azure Resource Locks Using ARM Templates

Overview
    â€¢ ARM templates automate deployment of Azure resources.
    â€¢ Can also apply resource locks during deployment.
    â€¢ Useful for preventing accidental deletion or modification.
    â€¢ Lock applies at control plane, not data plane.
    â€¢ Not a true security mechanism â€” users with RBAC can remove locks.

Template Name
    â€¢ Create a resourceGroup, apply a lock and RBAC
    â€¢ Found at: learn.microsoft.com â†’ Code Samples
    â€¢ Filters used: Azure Resource Manager, ARM, JSON
    â€¢ Search: lock

What the Template Does
    â€¢ Creates a resource group
    â€¢ Applies a CanNotDelete lock to it
    â€¢ Assigns the Contributor role to an Azure AD principal (Object ID)

Deploy via Azure Portal
    1. Click Deploy to Azure
    2. Fill:
        â—‹ Subscription
        â—‹ Region
        â—‹ rgName â†’ Resource group name (e.g., RgLockTest)
        â—‹ rgLocation â†’ Azure region (e.g., eastus)
        â—‹ principalId â†’ Azure AD Object ID (copied from user page)
        â—‹ roleDefinitionId â†’ Contributor role ID (auto-filled)
        â—‹ roleAssignmentName â†’ Optional, auto-generated
    3. Click Review + Create â†’ then Create

Editing the ARM Template
    1. Click Edit template at the top of the deployment screen
    2. Parameters:
        â—‹ rgName (string)
        â—‹ rgLocation (string)
        â—‹ principalId (string)
        â—‹ roleDefinitionId (string, default is Contributor ID)
        â—‹ roleAssignmentName (generated using guid() functions)
    3. Resources created:
        â—‹ Resource Group: Microsoft.Resources/resourceGroups
        â—‹ Lock: Microsoft.Authorization/locks
            Â§ name: DontDelete
            Â§ level: CanNotDelete
            Â§ notes: "Prevent deletion of the resourceGroup"
        â—‹ RBAC Assignment: Microsoft.Authorization/roleAssignments

Post-Deployment Validation
    1. Go to Resource groups > RgLockTest
    2. Under Access Control (IAM) â†’ Role Assignments:
        â—‹ Filter for username (e.g., Codey Blackwell)
        â—‹ Confirm Contributor role is assigned
    3. Under Locks:
        â—‹ Confirm lock name: DontDelete
        â—‹ Lock type: Delete
    4. Under Overview:
        â—‹ Location confirms deployment region (e.g., East US)
    5. Remember to click Refresh â€” UI may take time to reflect lock changes

Notes on PrincipalId
    â€¢ The PrincipalId is the Azure AD user's Object ID
    â€¢ Found under:
        â—‹ Azure AD â†’ Users â†’ Select user â†’ Object ID under "Basic Info"

Best Practices (AZ-500)
    â€¢ Use resource locks in ARM templates to enforce governance
    â€¢ Combine with role assignments for scoped access control
    â€¢ Document purpose of each lock via the notes field
    â€¢ Donâ€™t rely on locks as security â€” RBAC trumps it
    â€¢ Validate deployments post-creation (locks + role assignments)
    â€¢ Use ARM templates when deploying at scale or automating infrastructure

# Securing Az with Defender & Sentinel

## Microsoft Defender for Cloud

Overview
    â€¢ Microsoft Defender for Cloud (formerly Azure Security Center)
    â€¢ A unified cloud-native application protection platform (CNAPP) in Azure
    â€¢ Provides:
        â—‹ CSPM (Cloud Security Posture Management)
        â—‹ Workload Protection for servers, containers, databases, and more
    â€¢ Monitors:
        â—‹ Azure resources
        â—‹ On-premises infrastructure (via Azure Arc agent)
        â—‹ Multi-cloud platforms (AWS, GCP)
        â—‹ GitHub repositories
        â—‹ Microsoft 365 services

Capabilities
    â€¢ Continuous security assessment across environments
    â€¢ Flags misconfigurations and non-compliant resources
    â€¢ Provides actionable recommendations with guided remediation
    â€¢ Detects threats using Microsoft threat intelligence
    â€¢ Offers auto-provisioning of agents for supported services
    â€¢ Integrates with Logic Apps for automated incident response

Agent Requirements
    â€¢ Azure resources: monitored natively
    â€¢ On-prem or AWS/GCP VMs:
        â—‹ Must install Azure Arc agent
        â—‹ Enables telemetry and policy enforcement
    â€¢ Arc onboarding is needed before Defender coverage applies

Multi-Cloud Onboarding
    â€¢ To onboard AWS:
        â—‹ Provide:
            Â§ AWS Account ID
            Â§ Azure Subscription, Resource Group, and Location
            Â§ Connector name
        â—‹ Creates a Defender connector resource in Azure
    â€¢ Similar setup applies to GCP integration

Recommendations View
    â€¢ Findings sorted by severity:
        â—‹ High, Medium, Low
    â€¢ Each finding includes:
        â—‹ Unhealthy resource count
        â—‹ Remediation options:
            Â§ Manual fix
            Â§ One-click Fix
            Â§ Launch Logic App
            Â§ Exempt for acceptable risks

Sample Recommendations
    â€¢ Protect internet-facing VMs with NSGs
    â€¢ Enable Log Analytics agent
    â€¢ Restrict open NSG ports
    â€¢ Install endpoint protection
    â€¢ Enable:
        â—‹ Microsoft Defender for Containers
        â—‹ Microsoft Defender for Resource Manager
        â—‹ Microsoft Defender for App Service
        â—‹ Microsoft Defender for Key Vault
        â—‹ Secure transfer on Storage Accounts

Compliance Standards (Policy Initiatives)
    â€¢ Defender for Cloud can evaluate compliance against:
        â—‹ Microsoft Cloud Security Benchmark (MCSB)
        â—‹ PCI-DSS
        â—‹ ISO 27001
        â—‹ SOC TSP
    â€¢ Policies can be enabled or disabled
    â€¢ Defender assigns compliance scores per standard

Vulnerability Assessment Integration
    â€¢ Built-in vulnerability scanner (powered by Qualys)
    â€¢ Assesses:
        â—‹ Windows and Linux VMs
        â—‹ Container images (via Defender for Containers)
    â€¢ Displays:
        â—‹ Description of vulnerability
        â—‹ Severity
        â—‹ Remediation steps
        â—‹ Affected resources
        â—‹ Fix button / Exemption / Logic App trigger

Licensing
    â€¢ Free Tier:
        â—‹ Security posture management
        â—‹ Recommendations
    â€¢ Standard Tier (paid):
        â—‹ Threat protection
        â—‹ Alerts
        â—‹ Vulnerability assessments
        â—‹ Regulatory compliance dashboards

Best Practices (AZ-500)
    â€¢ Enable Microsoft Defender plans per resource type
    â€¢ Use Management Groups for Defender policy inheritance
    â€¢ Connect Arc-enabled servers for hybrid security posture
    â€¢ Regularly review compliance dashboard and threat alerts
    â€¢ Leverage auto-provisioning settings to enforce agent deployment
    â€¢ Use Logic Apps for automated remediation workflows

## Managing Microsoft Defender for Cloud for Azure Servers

Managing Microsoft Defender for Cloud for Azure Servers
ğŸ›¡ Purpose of Defender for Cloud
    â€¢ Detects vulnerabilities, threats, and misconfigurations
    â€¢ Applies continuous compliance checks against security benchmarks
    â€¢ Not limited to Azure: supports AWS, GCP, on-prem via Azure Arc

âš™ï¸ Storage Account Integration
    â€¢ Navigate: Storage Account â†’ Security + Networking â†’ Microsoft Defender for Cloud
    â€¢ Status: Shows if Defender for Storage is ON
    â€¢ Upgrade link available: Adds malware scanning + sensitive data discovery
Recommendations (with severity):
    â€¢ Use Private Link (Medium)
    â€¢ Restrict access via VNet rules
    â€¢ Disallow public access
    â€¢ Tactics/Techniques mapping (e.g. â€œInitial Accessâ€ for public exposure)
    â€¢ Remediation:
        â—‹ Quick Fix Logic: set allowBlobPublicAccess to false
        â—‹ Trigger Logic App for custom fixes
        â—‹ Assign issue to an owner with due date
        â—‹ Use Exempt if not applicable
        â—‹ View or assign policy definitions (e.g. deny effect to block future insecure configs)

âš™ï¸ Virtual Machine Integration
    â€¢ Navigate: VM â†’ Settings â†’ Microsoft Defender for Cloud
    â€¢ Displays:
        â—‹ of Recommendations
        â—‹ of Security Alerts
        â—‹ Defender for Servers status (e.g. ON)
        â—‹ Enable Just-in-Time VM Access (JIT): Reduces exposure window for admin ports
Example Recommendations:
    â€¢ High:
        â—‹ Enable Azure Disk Encryption
        â—‹ Encrypt temp disks + caches
        â—‹ Ensure updates check enabled
    â€¢ Medium/Low:
        â—‹ Enable Azure Backup
        â—‹ Install Log Analytics Agent
Actions:
    â€¢ View Remediation steps (manual links or auto-fix)
    â€¢ Use Take Action button to jump to config
    â€¢ View Security Incidents:
        â—‹ Show IPs, flags (malicious/unusual), timestamps

ğŸ§­ Central Console: Defender for Cloud
    â€¢ Access via Search â†’ Defender for Cloud
    â€¢ Overview:
        â—‹ Security score (e.g. 36%)
        â—‹ Cross-cloud monitoring (AWS, GCP)
    â€¢ Regulatory Compliance:
        â—‹ Microsoft Cloud Security Benchmark (e.g. 43/62 passed)
        â—‹ Drill into controls (e.g. PA-1: Privileged access separation)

ğŸ” Additional Navigation Panels
    â€¢ Security alerts
    â€¢ Inventory: Lists all resources (VMs, Storage, VNETs)
    â€¢ Add non-Azure servers
    â€¢ Security posture
    â€¢ Workload protections

AZ-500 Key Focus Areas
    â€¢ Defender plans: for Storage, for Servers, for App Services, etc.
    â€¢ Automation: Logic Apps, JIT, policy assignment
    â€¢ Threat detection, alert handling, and recommendation management
    â€¢ Cross-cloud + on-prem integration via Azure Arc
    â€¢ Understanding built-in policies and exemptions
    â€¢ Monitoring compliance with Microsoft Defender for Cloud dashboard


## Managing Microsoft Defender for Cloud for Databases


Managing Microsoft Defender for Cloud for Databases (AZ-500 Focused)

ğŸ¯ Purpose
    â€¢ Provide threat protection and vulnerability assessment for Azure and non-Azure databases.
    â€¢ Detects suspicious activity like:
        â—‹ SQL injection
        â—‹ Brute-force login attempts
        â—‹ Unauthorized data access
    â€¢ Scans configuration for security misconfigurations, compliance gaps, and missing best practices
    â€¢ Integrates with AWS, GCP, on-prem databases using Azure Arc

ğŸ”§ Enabling Defender for Databases
    1. Go to Microsoft Defender for Cloud â†’ Environment Settings
    2. Choose Management Group or your Azure Subscription
    3. Click Defender plans
    4. Scroll to Database section
    5. Toggle Defender ON, then click Select types:
        â—‹ Azure SQL Database âœ…
        â—‹ SQL servers on machines (on-prem or IaaS)
        â—‹ Azure Cosmos DB
        â—‹ Open-source relational DBs (e.g., MySQL, PostgreSQL)
    6. Save configuration

âœ‰ï¸ Alerts & Automation
    â€¢ Email Notifications:
        â—‹ Owner role notified by default
        â—‹ Can add other roles (e.g., Contributor) or custom emails
    â€¢ Workflow Automation:
        â—‹ Define triggers (e.g., High-severity alert)
        â—‹ Run Logic Apps to:
            Â§ Quarantine resources
            Â§ Auto-remediate issues
            Â§ Send alerts to SIEM or ticketing systems

ğŸ— Creating a Secure SQL Database (With Defender)
    1. Portal â†’ Create Resource â†’ Azure SQL â†’ Single Database
    2. Configure:
        â—‹ Resource group
        â—‹ Database name (e.g., app1sqldb1)
        â—‹ New/existing SQL Server
    3. Authentication:
        â—‹ SQL Authentication (Username + Password)
        â—‹ Optional: Azure AD integration
    4. Networking:
        â—‹ Use Private endpoint
        â—‹ Avoid public IP exposure
        â—‹ Assign to VNet/Subnet
    5. Security Tab:
        â—‹ Toggle Microsoft Defender for SQL â†’ "Start Free Trial" or "Enable"
    6. Proceed with:
        â—‹ Sample data
        â—‹ Tagging
        â—‹ Review + Create

ğŸ§ª Post-Deployment Monitoring
    â€¢ Go to SQL Database â†’ Microsoft Defender for Cloud
    â€¢ Under Security, view:
        â—‹ Recommendations (e.g., restrict public access)
        â—‹ Severity (High, Medium, Low)
        â—‹ Tactics/Techniques (e.g., Initial Access)
    â€¢ Use:
        â—‹ Quick Fix (auto sets allowBlobPublicAccess: false)
        â—‹ Assign Owner for follow-up
        â—‹ Exempt (optional, if justified)
    â€¢ Open View Policy Definition for granular control
    â€¢ Use Deny Policy Effect to prevent insecure deployments

ğŸ” Recommendation Examples
    â€¢ "Public access should be disabled"
    â€¢ "Use private endpoints"
    â€¢ "Enable advanced threat protection"
    â€¢ "Encrypt data at rest and in transit"
    â€¢ "Install vulnerability assessment extensions"

ğŸ§  Additional Details
    â€¢ Defender auto-assesses new + existing databases
    â€¢ Use View all recommendations to see tenant-wide security posture
    â€¢ Supports compliance monitoring (e.g., PCI DSS, ISO 27001, SOC TSP)
    â€¢ Defender checks apply regardless of deployment method (manual, ARM, Terraform)

âœ… Key AZ-500 Concepts
    â€¢ Microsoft Defender for SQL is part of Defender for Cloud
    â€¢ Protects both PaaS (Azure SQL DB) and IaaS-hosted SQL (via Arc agent)
    â€¢ Enables threat detection, policy assignment, and remediation workflows
    â€¢ Best practice: deploy using Private endpoints, enable Defender, and assign remediation automation>


## Viewing Microsoft Cloud Vulnerability Scan Results



Viewing Microsoft Cloud Vulnerability Scan Results (via Defender for Cloud)
Purpose
    â€¢ Microsoft Defender for Cloud scans Azure, AWS, GCP, and on-premises (via Azure Arc) for:
        â—‹ Vulnerabilities
        â—‹ Misconfigurations
        â—‹ Indicators of compromise (IoC)
Access Methods
    â€¢ View scan results at:
        â—‹ Individual resource level (e.g., VM, Storage Account)
        â—‹ Global view via Defender for Cloud dashboard

Resource-Level Scan (Example: Virtual Machine)
    â€¢ Navigate to VM > Defender for Cloud
        â—‹ View individual recommendations
        â—‹ Severity levels (High, Medium, Low)
        â—‹ Specific findings like â€œInstall endpoint protection,â€ â€œEncrypt disksâ€

Global Security Posture View
    â€¢ From Portal: Search Defender, open Microsoft Defender for Cloud
    â€¢ Overview:
        â—‹ Unified view of security recommendations across Azure and linked AWS/GCP accounts
        â—‹ Security Score
        â—‹ Assessed Resources Count

Environment Settings
    â€¢ View connected environments:
        â—‹ Azure subscriptions
        â—‹ External accounts (AWS/GCP)
    â€¢ Inventory includes EC2 instances, on-prem VMs, Azure resources

Inventory View
    â€¢ Filter by:
        â—‹ Cloud provider (Azure, AWS, GCP)
        â—‹ Resource Type (e.g., EC2, VM, Storage)
    â€¢ Drill down for:
        â—‹ Installed applications
        â—‹ Security recommendations
        â—‹ Affected resources
    â€¢ Export to CSV for external analysis (e.g., Excel filtering by resource type)

Regulatory Compliance
    â€¢ Microsoft Cloud Security Benchmark
    â€¢ Other Standards:
        â—‹ PCI DSS 3.2.1
        â—‹ ISO 27001
        â—‹ SOC TSP
    â€¢ View and download compliance reports:
        â—‹ Azure Shared Responsibility Matrix
        â—‹ Attestation of Compliance PDFs

Key Actions
    â€¢ Click Recommendations to view/fix misconfigurations
    â€¢ Use Audit Reports for compliance verification
    â€¢ Monitor AWS/GCP VMs like native Azure VMs
    â€¢ Download and manipulate CSV reports for documentation or audits

## Security Information and Event Management (SIEM) and Azure Sentinel

SIEM and Microsoft Sentinel (AZ-500 Study Notes)
SIEM Overview
    â€¢ SIEM = Security Information and Event Management
    â€¢ Purpose: Centralized threat detection, analysis, and response
    â€¢ Consolidates and correlates logs from multiple sources to detect anomalies, threats, and breaches
    â€¢ Core Functions:
        â—‹ Event aggregation (collects from many systems)
        â—‹ Correlation of events (identifies patterns)
        â—‹ Real-time alerting and dashboards
        â—‹ Forensic investigation support
        â—‹ Compliance reporting

SOAR Overview
    â€¢ SOAR = Security Orchestration, Automation, and Response
    â€¢ Extends SIEM with automated remediation and workflow orchestration
    â€¢ Can:
        â—‹ Run playbooks in response to alerts
        â—‹ Integrate with ticketing, email, or IP blocking systems
        â—‹ Allow human-in-the-loop or full automation

Microsoft Sentinel
    â€¢ Cloud-native SIEM + SOAR in Azure
    â€¢ Designed to analyze security data at scale with built-in AI/ML
    â€¢ Integrates with:
        â—‹ Microsoft Defender for Cloud
        â—‹ Microsoft 365 Defender
        â—‹ Azure AD
        â—‹ 3rd-party data sources (e.g., AWS, Barracuda, Cisco, Fortinet)

Core Components
1. Log Analytics Workspace
    â€¢ Foundation for Sentinel
    â€¢ All ingested data is stored here
    â€¢ Supports KQL (Kusto Query Language) for queries
2. Data Connectors
    â€¢ Prebuilt integrations for data ingestion
    â€¢ Examples:
        â—‹ Microsoft services: Azure AD, Defender, Office 365
        â—‹ 3rd-party: AWS CloudTrail, Palo Alto, Cisco ASA, Fortinet
        â—‹ Syslog: Generic log source for Linux/UNIX
    â€¢ Custom connectors supported via REST API or Logic Apps
3. Analytics Rules
    â€¢ Use built-in or custom rules to generate incidents from ingested data
    â€¢ Detect:
        â—‹ Unusual login behavior
        â—‹ Port scanning
        â—‹ Lateral movement
        â—‹ Exfiltration attempts
4. Incidents
    â€¢ Result of triggered analytics rules
    â€¢ Contain:
        â—‹ Timeline of related events
        â—‹ Entities involved (IP, user, hostname)
        â—‹ Severity & status (New, In Progress, Closed)
5. Workbooks
    â€¢ Dashboards for visualization
    â€¢ Customizable per scenario:
        â—‹ Threat hunting
        â—‹ Compliance reporting
        â—‹ SOC operations
6. Playbooks (SOAR)
    â€¢ Based on Azure Logic Apps
    â€¢ Respond automatically to incidents
    â€¢ Examples:
        â—‹ Disable user in Azure AD
        â—‹ Block IP in NSG
        â—‹ Send email/slack alert
        â—‹ Create ServiceNow ticket
7. Hunting
    â€¢ Manual threat investigation using KQL
    â€¢ Used by SOC analysts
    â€¢ Includes built-in hunting queries (MITRE ATT&CK mapped)
8. Entity Behavior Analytics (UEBA)
    â€¢ Identifies behavioral anomalies per user or host
    â€¢ Detects:
        â—‹ Impossible travel
        â—‹ Login location anomalies
        â—‹ Abnormal file access
9. Watchlists
    â€¢ External lists imported into Sentinel (IP blacklist, HR termination list, etc.)
    â€¢ Referenced in detection rules or queries

Use Cases
    â€¢ Ingest and correlate:
        â—‹ Azure VM logs, NSG flow logs
        â—‹ AWS CloudTrail events
        â—‹ Microsoft 365 login logs
    â€¢ Detect brute force, phishing, or insider threats
    â€¢ Auto-respond:
        â—‹ Quarantine VM
        â—‹ Disable account
        â—‹ Block IP on perimeter firewall

Integration Examples
Source	Method	Use In Sentinel
Azure AD logs	Built-in connector	Detect suspicious logins
AWS CloudTrail	Data connector + API keys	Monitor cloud activity
Linux servers	Syslog agent to Log Analytics	Monitor SSH activity, sudo, etc.
On-prem firewall	Common Event Format (CEF) agent	Ingest traffic logs, threat alerts
Defender for Endpoint	Native integration	Get device-level threats

Best Practices for AZ-500
    â€¢ Always use Log Analytics Workspace in the same region as resources
    â€¢ Enable MFA and monitor failed login attempts
    â€¢ Create custom analytics rules to suit your org
    â€¢ Use built-in templates for connectors and rules first
    â€¢ Configure Logic App-based Playbooks for SOAR
    â€¢ Use workbooks for executive dashboards
    â€¢ Regularly review incident timeline and severity
    â€¢ Enable UEBA for behavior-based detections

Microsoft Sentinel vs Other SIEMs
Feature	Microsoft Sentinel	Traditional SIEMs (e.g., Splunk)
Deployment	Fully cloud-native	On-prem or hybrid
Data ingestion	Azure-native & 3rd-party	Depends on integration effort
Scaling	Auto-scale with Azure	Manual provisioning
Pricing	Pay-as-you-go (GB ingested)	Often license-based
SOAR	Built-in (Logic Apps)	May need separate product/module

 
## Managing Azure Sentinel Connectors and Alerts


Managing Azure Sentinel Connectors and Alerts
1. Overview
    â€¢ Microsoft Sentinel must be attached to a Log Analytics workspace
    â€¢ Workspaces store ingested data, logs, incidents, alerts, and enable hunting with KQL
    â€¢ Data Connectors bring in telemetry from diverse sources
    â€¢ Alerts can trigger playbooks, notifications, or manual/automated incident response

2. Accessing Sentinel
    â€¢ Go to Azure Portal > Search: "Sentinel"
    â€¢ If not set up, create a Log Analytics Workspace
    â€¢ Attach Sentinel to it via "Add"

3. Data Connectors
    â€¢ Found under Configuration > Data connectors
    â€¢ 100+ built-in connectors for:
        â—‹ Azure services: AAD, Key Vault, NSG, Storage, etc.
        â—‹ Third-party sources: AWS, Cisco ASA, Barracuda, Fortinet, Palo Alto
        â—‹ On-prem devices: via Syslog, CEF (Common Event Format), REST APIs
Examples:
Connector Source	Data Types Ingested	Prerequisites
Azure Active Directory	Sign-in logs, audit logs, risky users	Azure AD diagnostic settings + proper roles (Global Admin)
Azure Storage Account	Blob read/write/delete logs	Configure diagnostic settings â†’ Log Analytics
NSG (Network Security Group)	Flow logs	Assign Azure Policy to send diagnostics to workspace
Cisco Meraki	Firewall/Security device logs via Syslog	Configure syslog export to Log Analytics
    âš ï¸ After free data ingestion quota (5GB/day as of writing), costs apply per GB, so only ingest what's needed

4. Steps to Connect a Data Source (e.g., Azure AD)
    1. Go to Sentinel > Data Connectors
    2. Click on source (e.g., Azure Active Directory)
    3. Review prerequisites (roles, diagnostics)
    4. Enable necessary logs (Sign-In, Audit, Risky Users, etc.)
    5. Apply changes â†’ Sentinel begins ingesting logs

5. Custom Diagnostic Settings for Storage
    1. Open Azure Storage Account
    2. Go to Monitoring > Diagnostic Settings
    3. Click Add Diagnostic Setting
    4. Choose Log Analytics workspace destination
    5. Enable relevant categories (Blob logs, etc.)
    6. Save

6. NSG Logs with Azure Policy
    â€¢ Some connectors (like NSG) require Azure Policy Assignment
    â€¢ Steps:
        1. Launch Policy Wizard from connector page
        2. Assign to subscription or resource group
        3. Select Log Analytics workspace
        4. Enable remediation task
        5. Create assignment â†’ NSG logs sent to Sentinel

7. Handling Third-Party Devices
    â€¢ E.g., Cisco Meraki
        â—‹ Needs Syslog configured
        â—‹ Sentinel provides instructions for log forwarding
        â—‹ Use Syslog/CEF collector VMs if needed

8. Sentinel Automation & Alerts
    â€¢ Found under Automation > Rules / Playbooks
    â€¢ Trigger response actions when:
        â—‹ Analytics rules fire
        â—‹ Specific incidents or thresholds are met
    â€¢ Actions can include:
        â—‹ Send email/Teams/Slack alert
        â—‹ Call Logic App playbook
        â—‹ Assign incident owner (e.g., Codey Blackwell)
        â—‹ Disable user or block IP

9. Hunting and Queries
    â€¢ Go to Hunting blade
    â€¢ Use KQL to:
        â—‹ Search for indicators of compromise (IoCs)
        â—‹ Investigate known campaigns (e.g., WannaCry DNS domains)
    â€¢ Select a query > Run selected query > View results

10. Best Practices
    â€¢ Regularly review connected connectors and data cost
    â€¢ Use filters to find connectors by vendor or type
    â€¢ Automate common alert responses with playbooks
    â€¢ Test queries in Hunting before creating new detection rules
    â€¢ Monitor ingestion costs post-trial and refine logs ingested
    â€¢ Review incident severity, assign owners, and triage frequently

AZ-500 Exam Tips
    â€¢ Know how to connect services to Sentinel using diagnostic settings
    â€¢ Understand prerequisites for major connectors like Azure AD, NSG
    â€¢ Be able to configure alert automation using Logic Apps
    â€¢ Be familiar with role requirements: Global Admin, Security Admin
    â€¢ Understand how to manage ingestion from on-prem (Syslog/CEF) and third-party


## Threat Modeling with the Microsoft Threat Modeling Tool


Threat Modeling with the Microsoft Threat Modeling Tool
1. Purpose
    â€¢ Helps IT admins, security engineers, developers visualize and secure app/data flows
    â€¢ Used to identify, analyze, and mitigate threats early in the development lifecycle
    â€¢ Free tool from Microsoft, designed for Windows

2. Setup
    â€¢ Download from Microsoftâ€™s official page
    â€¢ Requirements: Windows 10 Anniversary Update+, .NET 4.7.1+
    â€¢ Install via one-click setup
    â€¢ Launch the app and agree to license terms

3. Core Features
    â€¢ Supports Azure-specific templates (e.g., Azure Storage, Web Apps)
    â€¢ Drag-and-drop UI with components like:
        â—‹ Azure services (Storage, Web Apps, SQL)
        â—‹ Clients (Web browser, Mobile client, IoT)
        â—‹ Data flows (e.g., HTTP requests)

4. Workflow
Step-by-step:
    1. Open a template or start a new model
        â—‹ Example: "Azure Cloud Services" template
    2. Add components from the Stencils pane:
        â—‹ E.g., Azure Storage + Web Application + Request
    3. Configure Properties:
        â—‹ Azure Storage: type = Blob, enforce HTTPS
        â—‹ Web App: type = MVC or Web Forms
        â—‹ Data Flow: customize method (GET/POST), transport protocol
    4. Click View > Analysis View to run threat analysis
        â—‹ Tool lists identified threats automatically
        â—‹ Example threat: Unauthorized access to Azure Storage

5. Threat Analysis Output
Each threat includes:
Field	Description
Threat Name	E.g., Unauthorized access due to weak controls
Description	Explains how attacker might exploit the flaw
Mitigations	E.g., Use SAS (Shared Access Signature), enforce HTTPS, set RBAC properly

6. Model Management
    â€¢ Save models (e.g., SimpleAzureWebApp.tms)
    â€¢ Switch between Design View and Analysis View
    â€¢ Use File > Save or File > Export for documentation or audit trails

7. Advanced Modeling
    â€¢ Add more entities: Web Browser, Mobile Client, IoT Device, CRM
    â€¢ Mobile Client Technologies include:
        â—‹ Android, iOS, CRM Outlook Client, Dynamics Mobile
    â€¢ Define relationships and trust boundaries visually

8. Security Use Cases
Use Case	How Tool Helps
Azure Web App accessing Storage	Visualize flow, enforce HTTPS, restrict blob access
Client-server authentication flows	Model session tokens, credential storage, authorization gaps
IoT integration with cloud services	Analyze data integrity and communication exposure
Web APIs & third-party service usage	Spot over-permissive calls or weak auth flows

9. Benefits for Azure/AZ-500
    â€¢ Visualize attack surface of Azure-hosted apps/services
    â€¢ Identify issues before deployment
    â€¢ Understand use of mitigations like:
        â—‹ Shared Access Signatures
        â—‹ Role-Based Access Control (RBAC)
        â—‹ Network Security Group (NSG) limitations
    â€¢ Prepares for secure design questions on AZ-500

10. Best Practices
    â€¢ Use pre-built templates for cloud services when available
    â€¢ Always enforce HTTPS and proper access control in diagrams
    â€¢ Run analysis after all flows and assets are mapped
    â€¢ Document and export threat models for audit or review
    â€¢ Incorporate threat modeling into DevSecOps pipelines

## Managing Azure VM Updates



Managing Azure VM Updates
1. Why It Matters
    â€¢ Ensures critical security patches are applied
    â€¢ Prevents exploitation from unpatched OS vulnerabilities
    â€¢ Must balance security with stability/testing

2. Two Ways to Manage Updates
Method	Description
Per-VM Manual	Through individual VM settings in Azure portal
Automation Account	Centralized update management via Log Analytics & Update Management

3. Manual Updates (Per VM)
Steps:
    1. Go to Virtual machines in Azure Portal
    2. Select a VM (Linux or Windows)
    3. In left nav: under Operations, click Updates
    4. Click Check for updates (if needed)
Options:
    â€¢ One-time update: Apply now
    â€¢ Classifications: Filter by Security, Critical, etc.
    â€¢ View update list by:
        â—‹ Name/version
        â—‹ Category
        â—‹ Count (e.g., 86 total updates, 60 critical)
    â€¢ Reboot options:
        â—‹ Reboot if required
        â—‹ Never reboot
        â—‹ Always reboot
    â€¢ Maintenance window: Duration (in minutes) Azure has to apply updates
Scheduling:
    â€¢ You can also click Schedule update for recurring deployments

4. Automation Account + Update Management (Recommended at scale)
4.1. Create Automation Account
    1. Azure Portal â†’ search Automation Account â†’ Create
    2. Fill in:
        â—‹ Name (e.g., automation1)
        â—‹ Region (e.g., East US)
        â—‹ Identity: System-assigned
        â—‹ Public access allowed
        â—‹ No Tags (optional)
    3. Click Create â†’ Go to resource

4.2. Enable Update Management
    â€¢ In the Automation Account:
        1. Click Update Management in left nav
        2. Link to a Log Analytics Workspace
            Â§ Can use existing or create new
        3. Click Enable
        4. After enabling, refresh the screen

4.3. Add Virtual Machines
    â€¢ Click Add Azure VMs
    â€¢ Select VMs to monitor (Windows/Linux)
    â€¢ Click Enable
    Can also add non-Azure machines via Azure ARC

5. Schedule Update Deployment
After VMs are added:
    â€¢ Click Schedule update deployment
    â€¢ Options include:
        â—‹ Update classification (Security, Critical, etc.)
        â—‹ Include/Exclude specific updates (e.g., by KB ID)
        â—‹ Reboot settings
        â—‹ Maintenance window
        â—‹ Recurring schedule

6. Benefits of Using Automation Account
Feature	Benefit
Centralized control	Manage updates for 100s of VMs from one place
Reporting	See compliance and missing updates in Log Analytics
Supports hybrid environments	Works for Azure VMs + on-premises via ARC
Integration with Security	Helps meet compliance/audit standards (e.g., PCI, NIST)

7. Best Practices
    â€¢ Always test updates in dev/staging before prod
    â€¢ Schedule updates during maintenance windows
    â€¢ Set "Reboot if required" for safer automation
    â€¢ Monitor compliance using Log Analytics queries

8. AZ-500 Relevance
    â€¢ Understanding Update Management is key to:
        â—‹ Maintaining secure posture
        â—‹ Managing hybrid cloud security
        â—‹ Automating remediation as part of SOAR
    â€¢ May be tested on:
        â—‹ VM update compliance
        â—‹ Automation Account setup
        â—‹ Linking Log Analytics

# Monitoring Az Services

 ## Working with Action Groups

 1. Purpose of Action Groups
    â€¢ Action Groups define how Azure responds when an alert rule triggers.
    â€¢ They are reusable notification/action bundles used by Azure Monitor and security alerting systems.
    â€¢ Used in scenarios such as:
        â—‹ Security incidents (e.g., unauthorized access, resource abuse).
        â—‹ Performance degradation (e.g., high CPU from malware).
        â—‹ Compliance violations (e.g., untagged resources, disabled firewalls).
 
 2. Key Components of an Action Group
 Each action group can have notifications and automated actions:
 a. Notification Types
    â€¢ Email
    â€¢ SMS
    â€¢ Push notifications (Azure Mobile App)
    â€¢ Voice call
 b. Action Types
    â€¢ Automation Runbook â€“ triggers remediation scripts.
    â€¢ Logic App â€“ complex workflows and integrations.
    â€¢ Azure Function â€“ custom code execution.
    â€¢ Webhooks / Secure Webhooks â€“ external system integration.
    â€¢ ITSM â€“ creates tickets in ServiceNow or other ITSM tools.
    â€¢ Event Hub â€“ stream alert data to SIEM/SOAR platforms.
 
 3. Creating an Action Group
 Steps to create in the Azure Portal:
    1. Azure Monitor > Alerts > Action Groups > Create
    2. Basics:
        â—‹ Name, Region, Resource Group
        â—‹ Note: Action Groups are global, not tied to a specific resource.
    3. Notifications:
        â—‹ Add one or more notification methods (e.g., email and SMS).
    4. Actions:
        â—‹ Choose optional automation: runbooks, Logic Apps, etc.
    5. Tags (optional):
        â—‹ Useful for tracking in cost management or resource management.
    6. Review + Create
 
 4. Using Action Groups in Alert Rules
 a. Alert Rule Setup
    â€¢ Navigate to any resource (e.g., VM, App Service).
    â€¢ Go to Monitoring > Alerts > Create Alert Rule.
    â€¢ Define:
        â—‹ Scope (resource)
        â—‹ Condition (signal + threshold, e.g., CPU > 80%)
        â—‹ Action Group (create new or select existing)
        â—‹ Alert Details (name, severity, description)
 b. Evaluation Frequency
    â€¢ Example:
        â—‹ Check every 1 minute
        â—‹ Lookback period: 5 minutes
 c. Alert Logic Examples
    â€¢ Web App HTTP 5xx errors > 1
    â€¢ VM CPU % > 80
    â€¢ Firewall rule deleted (via activity logs)
 
 5. Integration with Security & Compliance
    â€¢ Azure Security Center/Defender for Cloud integrates with Action Groups to notify on:
        â—‹ Regulatory compliance issues
        â—‹ Just-in-Time VM access requests
        â—‹ Threat detection alerts
    â€¢ Use Logic Apps for advanced incident response (e.g., isolate VM, disable account).
 
 6. Managing Action Groups Across Resources
    â€¢ Action groups are reusable across:
        â—‹ VMs, App Services, Key Vaults, SQL, Cosmos DB, Storage
    â€¢ Central visibility under:
 Azure Monitor > Alerts > Action Groups
    â€¢ Recommended to use consistent naming conventions (e.g., SecOps-Notify, AutoRemediate-Critical).
 
 7. Exam-Relevant Notes
    â€¢ Action groups can be global and are NOT resource-specific.
    â€¢ Alert rules use action groups to define notification + response.
    â€¢ Multiple action groups can be attached to one alert rule.
    â€¢ Secure webhook uses OAuth2 authentication (for secure external calls).
    â€¢ Tags help classify and organize action groups but are not mandatory.
    â€¢ Understand integration with:
        â—‹ Activity Logs: Alert on create/delete resource events
        â—‹ Log Analytics: Custom queries for alert conditions (Kusto query language)
  
 ## Configuring Alert Notification

 1. Purpose
    â€¢ Detect abnormal or risky conditions in Azure resources (e.g., CPU spikes, network anomalies, service failures).
    â€¢ Trigger notifications or automated response actions.
    â€¢ Enhance visibility, response, and compliance with operational and security events.
 
 2. Access Azure Monitor
    â€¢ Go to Azure Portal > Monitor
    â€¢ Core areas:
        â—‹ Overview
        â—‹ Alerts
        â—‹ Metrics
        â—‹ Activity Log
        â—‹ Insights (VMs, Apps, Containers, Key Vaults, etc.)
 
 3. Enable VM Monitoring
    â€¢ Monitor > Virtual Machines
    â€¢ Use Configure Insights:
        â—‹ Chooses Azure Monitor Agent (default)
        â—‹ Applies Data Collection Rule (DCR)
        â—‹ Monitored VMs show up under "Monitored"; others under "Not Monitored"
    âš ï¸ VM must be powered on to configure insights.
 
 4. Create an Alert Rule
 Steps:
    1. Scope â€“ select resource (e.g., Linux1)
    2. Condition â€“ define alert logic
 Example:
        â—‹ Signal: Percentage CPU
        â—‹ Aggregation: Average
        â—‹ Operator: >
        â—‹ Threshold: 80% (or lower to test)
    3. Evaluation Period:
        â—‹ Frequency: every 1 min
        â—‹ Lookback: 5 mins
    4. Action Groups â€“ select one or more
    5. Details â€“ name, severity (0â€“4), description
    6. Tags â€“ optional, for filtering and cost tracking
    7. Review + Create
 
 5. Edit Alert Rule (Post-Creation)
    â€¢ Go to Alerts > Alert Rules
    â€¢ Click existing rule (e.g., Linux1CPU)
    â€¢ Use Edit to:
        â—‹ Add/remove action groups
        â—‹ Change condition thresholds
        â—‹ Modify alert logic
 
 6. Create Additional Alert Rule (New Metric)
    â€¢ Example:
        â—‹ Signal: Network In Total
        â—‹ Threshold: > 500 MB
        â—‹ Purpose: Detect abnormal data transfer
    â€¢ Reuse existing action groups (TextAdmins, EmailAdmins)
 
 7. Create Action Groups
 Example: EmailAdmins
    1. Go to Monitor > Alerts > Action Groups > Create
    2. Basics: Name, region, resource group
    3. Notifications:
        â—‹ Type: Email/SMS/Push/Voice
        â—‹ Add email (gets "welcome" message from Azure)
    4. Actions (optional):
        â—‹ Skip or add: Runbook, Logic App, Azure Function, Webhook
    5. Enable Common Alert Schema (for SIEM/SOAR compatibility)
    6. Review + Create
    ğŸ” Action groups are global, reusable across alert rules and resources.
 
 8. Use Multiple Action Groups
    â€¢ Add more than one group to a single alert rule.
    â€¢ Example:
        â—‹ TextAdmins (SMS)
        â—‹ EmailAdmins (Email only)
    â€¢ Flexibility in who gets notified and how.
 
 9. How Notifications Are Sent
    â€¢ Email: Confirmation + alert trigger
    â€¢ SMS: Short alert summary (e.g., â€œSev3 alert: Linux1CPUâ€)
    â€¢ Azure App: Push notification
    â€¢ Voice: Robo-call with alert message
 
 10. Best Practices (AZ-500 Specific)
    â€¢ Use descriptive alert names (e.g., Linux1CPUThreshold, App1-HTTP-Errors).
    â€¢ Group alerts by severity to match response SLAs.
    â€¢ Use tags to track alerting rules by owner, environment, or business unit.
    â€¢ Enable Common Alert Schema for uniform formatting across tools.
    â€¢ Integrate with:
        â—‹ Log Analytics for custom query-based alerts
        â—‹ Event Hubs for SIEM ingestion
        â—‹ Logic Apps for automated remediation
        â—‹ ITSM connectors for ticket creation (e.g., ServiceNow)
  
 # Ensuring buisness continuity 

 ## Enabling Web App Application Insights
 
1. Purpose of Application Insights
    â€¢ Deep performance monitoring, availability checks, and usage analytics for web apps.
    â€¢ Detects failures, performance bottlenecks, and user behavior patterns.
    â€¢ Supports custom telemetry via SDK integration.

2. Application Insights Supported Platforms
    â€¢ Works best with:
        â—‹ .NET, .NET Core
        â—‹ Node.js, Java
    â€¢ Not available for:
        â—‹ Some Linux-based runtime stacks (e.g., Python)
        â—‹ You must use supported runtimes for full integration.

3. Deploying a Web App with Application Insights
a. Go to:
Azure Portal > App Services > Create
b. Basic Setup
    â€¢ Name: e.g., samplenewandwonderfulapp
    â€¢ Platform: Windows
    â€¢ Stack: .NET Core or similar
    â€¢ Region: Same as Application Insights (or let Azure create new)
c. Monitoring Tab
    â€¢ Enable Application Insights: Yes (default for supported stacks)
    â€¢ Select AI resource:
        â—‹ Use existing
        â—‹ Or let Azure create a new resource for this web app
d. Finalize
    â€¢ Click Review + Create, then Create

4. After Deployment
a. Navigate to the Web App
    â€¢ In left pane, Application Insights is now visible
    â€¢ Link is shown under Monitoring > Application Insights
b. First-time Setup
    â€¢ Choose:
        â—‹ Collection Level: Recommended
        â—‹ Enable Profiler (optional)
        â—‹ Snapshot Debugger / SQL Monitoring (optional)
    â€¢ Click Apply, confirm restart of app

5. Exploring Application Insights Features
a. Application Map
    â€¢ Visual dependency map of app components
    â€¢ Shows number of calls, latency, and failures
    â€¢ Useful for tracing service dependencies and slow operations
b. Performance
    â€¢ View request duration, frequency, and response time trends
    â€¢ Breakdown by operation, dependency, or role
    â€¢ Compare durations and identify long-running requests
c. Live Metrics
    â€¢ Near real-time view of:
        â—‹ Incoming/outgoing requests
        â—‹ Response time
        â—‹ Server health
        â—‹ Memory usage
d. Availability
    â€¢ Track uptime using availability tests
    â€¢ Can create synthetic ping tests from global test agents
    â€¢ View results: % availability, failures, locations
e. Failures
    â€¢ Detect:
        â—‹ HTTP 4xx/5xx errors
        â—‹ Exception types
        â—‹ Failed dependencies (e.g., DB or API calls)

6. Additional Monitoring Options
a. Alerts
    â€¢ Navigate: Monitoring > Alerts
    â€¢ Create alert rules based on metrics like:
        â—‹ Server exceptions
        â—‹ Server response time
        â—‹ Failed request count
b. Metrics Blade
    â€¢ View custom metrics:
        â—‹ Page load time
        â—‹ Server response time
        â—‹ Dependency call duration
c. Application Dashboard
    â€¢ Auto-generated overview with tabs for:
        â—‹ Usage (users, sessions)
        â—‹ Reliability (failures, success rate)
        â—‹ Responsiveness (request duration)
        â—‹ Browser insights (user environment)

7. Best Practices for AZ-500
    â€¢ Use Application Insights for continuous monitoring of mission-critical apps.
    â€¢ Combine with Alerts + Action Groups to automate response.
    â€¢ Use Live Metrics and Snapshot Debugger for fast troubleshooting.
    â€¢ Integrate with Log Analytics for advanced querying (Kusto Query Language).
    â€¢ Ensure Monitoring is in place for all production workloads for both security and operational readiness.

## Managing Log Analytic Sources

1. Purpose of Log Analytics
    â€¢ Centralized querying and analysis platform for log and telemetry data.
    â€¢ Powered by Azure Monitor Logs, built on Kusto Query Language (KQL).
    â€¢ Supports security, performance, and compliance monitoring across services.

2. Log Analytics Workspace
    â€¢ Container where log data is collected and stored.
    â€¢ Resources send telemetry to a workspace.
    â€¢ You can connect:
        â—‹ Azure VMs
        â—‹ Azure PaaS resources
        â—‹ On-premises systems (via agents)
        â—‹ Diagnostics settings from other Azure services
Workspace Properties:
    â€¢ Name
    â€¢ Region (must match data sources in many cases)
    â€¢ Retention policy
    â€¢ Linked with Defender for Cloud, Sentinel, Monitor

3. Supported Data Sources
a. Azure Resources
    â€¢ VMs (via Azure Monitor Agent)
    â€¢ App Services
    â€¢ Key Vault
    â€¢ Storage Accounts
    â€¢ Network Security Groups (NSGs)
    â€¢ Azure Firewall
    â€¢ Application Gateway
    â€¢ Azure SQL
b. Custom Logs
    â€¢ Upload .log files or use custom-defined schema.
    â€¢ Parse with regular expressions or delimiters.
c. Agents
    â€¢ Azure Monitor Agent (AMA) â€“ current standard.
    â€¢ Log Analytics Agent (MMA/OMS) â€“ legacy, being deprecated.

4. Connect Data Sources to Workspace
a. Azure VM
    1. Go to Monitor > Virtual Machines
    2. Click Enable
    3. Select existing workspace or create one
    4. Uses Data Collection Rule (DCR) if using AMA
b. PaaS Services
    â€¢ Go to resource (e.g., Storage Account)
    â€¢ Navigate to Diagnostic Settings
    â€¢ Click Add diagnostic setting
    â€¢ Choose:
        â—‹ Log types (e.g., Read/Write/Delete requests)
        â—‹ Metrics
        â—‹ Destination: Log Analytics, Event Hub, Storage

5. Diagnostic Settings
    â€¢ Define what data is sent and where it goes
    â€¢ Up to 5 diagnostic settings per resource
    â€¢ Can send to:
        â—‹ Log Analytics
        â—‹ Event Hub
        â—‹ Storage Account
    ğŸ” For security, always log:
        â—‹ Admin operations
        â—‹ Authentication attempts
        â—‹ Network changes

6. Log Retention & Management
    â€¢ Default retention: 30 days
    â€¢ Can be configured per workspace
    â€¢ Older data incurs additional storage cost
    â€¢ Use Data Export rules to move logs to storage

7. Querying Logs with KQL
    â€¢ Go to Logs under the workspace or resource
    â€¢ Use tables like:
        â—‹ Heartbeat, Perf, SecurityEvent, AzureActivity, AppRequests
    â€¢ Sample query:
SecurityEvent
| where TimeGenerated > ago(1d)
| summarize count() by EventID

8. Best Practices (AZ-500 Relevant)
    â€¢ Use centralized Log Analytics workspace for visibility across tenants/subscriptions.
    â€¢ Configure diagnostic settings for all critical resources (Storage, NSGs, Key Vaults).
    â€¢ Use Data Collection Rules for granular control of what logs get sent.
    â€¢ Integrate with Microsoft Sentinel for threat hunting and incident response.
    â€¢ Enforce access control (RBAC) on workspaces to protect sensitive logs.
    â€¢ Enable retention policies and data export for compliance 

# Ensuring Business Continuity
## Azure Backup Solutions 


Azure Backup Solutions (AZ-500 Focus)
1. Why Backups Matter
    â€¢ Protection against data loss (accidental deletion, corruption, ransomware).
    â€¢ Ensure business continuity and meet regulatory compliance.
    â€¢ Back up:
        â—‹ Data (files, VMs, DBs)
        â—‹ Service configurations (e.g., app settings, network config)

2. Key Concepts
Term	Meaning
RPO (Recovery Point Objective)	Max data loss allowed (e.g., â€œ1 hour of ordersâ€)
RTO (Recovery Time Objective)	Max downtime allowed (e.g., â€œservice must recover within 20 minutesâ€)
    ğŸ” RPO drives backup frequency, RTO drives restore speed

3. Azure Backup Capabilities
Azure Backup supports:
    â€¢ Azure VMs (entire VM snapshots)
    â€¢ SQL Server on Azure VMs
    â€¢ Azure Files and Azure Blobs
    â€¢ On-premises machines via MARS or Azure Backup Server
    â€¢ Azure Managed Disks
    â€¢ App service configuration backups (e.g., Web App settings)
    âœ… Supports encryption, soft delete, multi-region storage, and retention policies

4. High Availability vs Backup
Feature	Purpose
Backup	Point-in-time copy of data for recovery
High Availability (HA)	Ensures uptime and data accessibility
Disaster Recovery (DR)	Enables full service replication to alternate location (e.g., Azure Site Recovery)

5. Backup Frequency Planning
    â€¢ Depends on RPO:
        â—‹ RPO = 1 hour â†’ back up hourly
        â—‹ RPO = 10 minutes â†’ use continuous backup (e.g., SQL Transaction Logs)
    â€¢ Not one RPO/RTO per org â€” they vary per workload

6. Retention Planning
    â€¢ Set short-term (daily, weekly) and long-term (monthly, yearly) retention
    â€¢ Meets compliance requirements (HIPAA, GDPR, etc.)
    â€¢ Immutable backup options protect against tampering

7. Storage Location & Compliance
    â€¢ Backups stored in:
        â—‹ Azure Recovery Services Vault
        â—‹ Azure Backup Vault (modern, RBAC-enabled)
    â€¢ Choose:
        â—‹ Locally Redundant Storage (LRS) â€“ cost-effective
        â—‹ Geo-Redundant Storage (GRS) â€“ for DR across regions
    ğŸŒ Data residency matters for compliance: choose regions wisely

8. Azure Site Recovery (ASR)
    â€¢ Disaster Recovery as a Service (DRaaS)
    â€¢ Replicates:
        â—‹ On-prem physical servers
        â—‹ VMs (Azure or on-prem)
        â—‹ VMWare/Hyper-V workloads
    â€¢ Supports failover and failback for business continuity

9. VM & Service Redundancy
    â€¢ VM replication across regions
    â€¢ Use Geo-redundant Storage (GRS) for data
    â€¢ Use App Service Deployment Slots for zero-downtime updates
    â€¢ Use read-only DB replicas for cross-region failover/performance

10. Load Balancing & Fault Tolerance
    â€¢ Distribute traffic across multiple backend VMs
    â€¢ Auto-detects and excludes unhealthy VMs
    â€¢ Supports scalability and resilience

11. Example Scenarios
Service	RPO	RTO	Notes
E-commerce payment	5 mins	15 mins	Very critical
Internal documentation server	4 hrs	6 hrs	Low urgency
Customer orders DB	10 mins	30 mins	High-priority

12. Best Practices (AZ-500 Relevant)
    â€¢ Use Recovery Services Vaults with locked soft delete
    â€¢ Encrypt backups with customer-managed keys (CMK) if needed
    â€¢ Set role-based access control (RBAC) on vaults
    â€¢ Test restore operations regularly
    â€¢ Configure alerts for backup failures
    â€¢ Combine Azure Backup and ASR for full protection


## Enabling Virtual Machine Replication




1. Objective
    â€¢ Enable Disaster Recovery for Azure VMs using Azure Site Recovery (ASR).
    â€¢ Replicate VM disks and configurations to a secondary Azure region.
    â€¢ Provides business continuity in case of region-wide failure or planned failover.

2. Terminology
Term	Description
ASR (Azure Site Recovery)	Azureâ€™s Disaster Recovery as a Service (DRaaS)
Primary Region	Source region where original VM resides
Secondary Region	Target region for replica deployment
Replication Health	Indicates sync status and issues
Failover	Switch operations from primary VM to secondary replica
Test Failover	Simulate failover without impacting production
Cleanup Test Failover	Removes test VM and validates failover process
RPO (Recovery Point Objective)	Max tolerable data loss, shown in minutes
RTO (Recovery Time Objective)	Max tolerable downtime, used in planning

3. Pre-requisites
    â€¢ A running Azure VM in a supported region.
    â€¢ Azure VM must use managed disks.
    â€¢ Disaster Recovery must be enabled via the Azure portal or Azure CLI.
    â€¢ VM should have Site Recovery extension installed (automatically handled).

4. Enabling Replication
a. Portal Navigation:
    1. Go to Virtual Machines > [VM Name]
    2. Click Disaster Recovery under Operations
b. Basics Tab:
    â€¢ Set Disaster Recovery between zones to No (for cross-region failover)
    â€¢ Select Target Region (e.g., West Central US)
c. Advanced Settings Tab:
    â€¢ Default:
        â—‹ Replica Resource Group: [source-name]-asr
        â—‹ Replica VNet: auto-created in target region
    â€¢ Disk Type: Match source (e.g., Premium SSD)
    â€¢ Cache storage & churn threshold are customizable
d. Review + Start Replication:
    â€¢ Start the process
    â€¢ Azure provisions:
        â—‹ Replica disk(s)
        â—‹ Replica network
        â—‹ Recovery Services resources

5. Post-Replication Validation
After deployment completes:
    â€¢ Navigate to Disaster Recovery for the VM
    â€¢ Verify:
        â—‹ Replication Health = Healthy
        â—‹ Status = Protected
        â—‹ RPO = ~ few minutes
        â—‹ Agent = Healthy
    â€¢ Failover and Test Failover buttons will become active after full sync.

6. Test Failover Process
    1. Click Test Failover
    2. Select Recovery Point (latest or previous snapshot)
    3. Choose Replica VNet
    4. Azure creates and boots a temporary VM in the secondary region
    5. Use it to validate disaster recovery plan
    6. Click Cleanup Test Failover after confirmation

7. Failover Operation
    â€¢ Used during:
        â—‹ Regional outages
        â—‹ Disaster recovery scenarios
        â—‹ Unrecoverable service disruption
Steps:
    1. Click Failover
    2. Select Recovery Point
    3. Initiate failover â†’ ASR boots replica VM in target region
    4. Optionally commit failover (make permanent) or fail back

8. Failover Readiness Monitoring
    â€¢ Check Last successful test failover
    â€¢ Agent version & status must be current
    â€¢ Address any configuration issues shown in portal

9. Azure Resource Group Management
    â€¢ ASR creates a new resource group (e.g., app1-asr)
    â€¢ Contains:
        â—‹ Replica disks
        â—‹ Replica VNet
        â—‹ Supporting infra for DR

10. Best Practices (AZ-500 Focus)
    â€¢ Use Geo-redundant storage on VM disks when possible
    â€¢ Perform Test Failover at least quarterly
    â€¢ Monitor RPO metrics via portal or Log Analytics
    â€¢ Combine ASR with Azure Backup for full protection
    â€¢ Protect all mission-critical VMs in production
    â€¢ Document DR plans and perform periodic drills
    â€¢ Enable Alerts for replication health or failures

11. Key Considerations
    â€¢ Replication incurs additional cost (compute/storage/network)
    â€¢ Failover VMs can be renamed or re-IPâ€™ed post-failover
    â€¢ Not all VM SKUs are supported in all regions â€” check region pairing
    â€¢ ASR doesnâ€™t replicate:
        â—‹ External dependencies (e.g., DNS config)
        â—‹ Certificates stored outside the VM
 

##  Backing Up Azure Virtual Machines



1. Purpose
    â€¢ Use Azure's cloud-native backup service to protect Azure VMs.
    â€¢ Supports VM-level, disk-level, and file-level restores.
    â€¢ Backup configurations are managed via a Recovery Services vault.

2. Core Component: Recovery Services Vault
    â€¢ Logical container to manage:
        â—‹ Backup items
        â—‹ Policies
        â—‹ Replicated VMs (ASR)
    â€¢ Deployed per-region, associated with subscriptions.

3. Backup Process Overview
    1. Create or use existing Recovery Services vault.
    2. Configure backup source: e.g., Azure VM.
    3. Assign or create backup policy (schedule + retention).
    4. Select target VM.
    5. Enable backup.
    6. Monitor backup and trigger on-demand backup if needed.

4. Initiating Backup (Step-by-Step)
a. Go to:
Azure Portal > Recovery Services vaults > [Vault Name] > Backup
b. Select:
    â€¢ Where is your workload running? â†’ Azure
    â€¢ What do you want to back up? â†’ Virtual machine
c. Configure Backup:
    â€¢ Choose:
        â—‹ Policy Type: Standard (1x/day) or Enhanced (multiple/day)
        â—‹ Use DefaultPolicy or create a custom policy
d. Add VMs:
    â€¢ VMs must be in the same region as the vault
    â€¢ Can optionally exclude data disks
e. Click Enable Backup

5. Backup Policies
    â€¢ Set:
        â—‹ Frequency: Daily or Weekly
        â—‹ Time: When to run backup
        â—‹ Retention: Daily, Weekly, Monthly, Yearly
        â—‹ Instant Recovery Snapshots (for quick restores)
    ğŸ’¡ Custom policies offer more flexibility for RPO/RTO alignment

6. Monitoring & Validation
    â€¢ Navigate to:
        â—‹ Vault > Backup items
        â—‹ View:
            Â§ Backup item count
            Â§ Pre-check status
            Â§ Last backup status
    â€¢ Or:
        â—‹ VM > Backup (under Operations)

7. Initial Backup
    â€¢ Until the first backup completes, restore actions are disabled.
    â€¢ You can click Backup Now to trigger manually.
    â€¢ Default retention: 1 month (customizable).

8. Restore Options
    â€¢ Restore VM:
        â—‹ Restores entire VM to a new VM or original location
    â€¢ File Recovery:
        â—‹ Mounts recovery disk temporarily to extract specific files
    ğŸ›¡ Restore points are created daily or per policy.

9. On-Premises Workloads
    â€¢ Recovery Services vault also supports:
        â—‹ Windows/Linux file servers
        â—‹ Hyper-V/VMware
        â—‹ SQL Server, SharePoint, Exchange
    â€¢ Requires Microsoft Azure Recovery Services Agent (MARS)
    â€¢ Download:
        â—‹ Agent software
        â—‹ Vault credentials (for authentication)

10. Security & Governance (AZ-500 Specific)
    â€¢ Backup data is:
        â—‹ Encrypted at rest
        â—‹ Can use Customer-Managed Keys (CMK)
    â€¢ Soft delete protects against accidental deletions
    â€¢ RBAC enforces access control for backup management
    â€¢ Alerts/logs can be integrated into Azure Monitor or Sentinel

11. Best Practices
    â€¢ Use enhanced policies for mission-critical VMs
    â€¢ Regularly test Restore VM and File Recovery
    â€¢ Keep vault and VM in same region
    â€¢ Use tags for tracking backup scope
    â€¢ Audit using:
        â—‹ Azure Activity Logs
        â—‹ Backup reports in Log Analytics

 
## Managing Azure SQL Backups


1. Overview
    â€¢ Azure SQL Database provides automatic backups by default.
    â€¢ Supports:
        â—‹ Point-in-Time Restore (PITR)
        â—‹ Long-Term Retention (LTR)
    â€¢ Backup configuration is managed at the SQL server level, not the database level.

2. Backup Redundancy Settings
    â€¢ Configure during database creation or after.
    â€¢ Navigate to:
SQL Database > Settings > Compute + Storage > Backup Storage Redundancy
Options:
        â—‹ Locally Redundant Storage (LRS)
        â—‹ Zone Redundant Storage (ZRS)
        â—‹ Geo Redundant Storage (GRS) (default)
    GRS enables backups to be replicated to a paired region for disaster recovery.

3. Encryption
    â€¢ Transparent Data Encryption (TDE) is ON by default:
        â—‹ Encrypts data files, logs, and backups
        â—‹ Can use Microsoft-managed or customer-managed keys (CMK)

4. Accessing Backup Settings
    â€¢ Open SQL Server, not just the Database
    â€¢ Go to:
SQL Server > Data Management > Backups

5. Point-in-Time Restore (PITR)
    â€¢ Enabled by default.
    â€¢ Default retention: 7 days
    â€¢ Adjustable up to 35 days
    â€¢ Navigate to:
Backups > Retention Policies > Configure Policies
    PITR helps meet short-term recovery goals (low RPOs).

6. Differential Backups
    â€¢ Default frequency: every 12 hours
    â€¢ Changeable up to every 24 hours
    â€¢ Tracks changes since last full backup (optimized storage & performance)

7. Long-Term Retention (LTR)
    â€¢ Separate policy from PITR
    â€¢ Store weekly/monthly/yearly backups for up to 10 years
    â€¢ Use when:
        â—‹ Compliance requires long-term backup retention (e.g., HIPAA, GDPR)
Configure in:
SQL Server > Backups > Retention Policies > Configure Policies

8. Deleted Databases
    â€¢ Navigate to:
SQL Server > Data Management > Deleted Databases
    â€¢ You can restore recently deleted databases if within retention window.

9. Manual Backup Not Needed
    â€¢ Azure SQL Database handles all backup scheduling, storage, encryption.
    â€¢ Admins only configure policiesâ€”not perform actual backups.

10. Backup Limitations
    â€¢ Only logical backupsâ€”no direct access to *.bak files
    â€¢ Not suitable for native SQL Server restore workflows
    â€¢ Can't use Recovery Services Vault for managed Azure SQL Database

11. Recovery Services Vault: For Azure SQL in IaaS (VMs)
    â€¢ Navigate to:
Recovery Services Vault > Backup > Azure > SQL Server in Azure VM
    â€¢ Requires:
        â—‹ Agent installed on VM
        â—‹ Vault credentials for auth
        â—‹ Manual configuration of backup policies

12. On-Prem SQL Server Backups
    â€¢ Select:
        â—‹ Where is workload running? â†’ On-Premises
        â—‹ What do you want to back up? â†’ Microsoft SQL Server
Steps:
    1. Install Azure Backup Server (MABS)
    2. Download vault credentials
    3. Configure backup on-prem via MABS UI

13. Security & Compliance (AZ-500 Relevance)
    â€¢ Role-based Access Control (RBAC) manages backup config access.
    â€¢ Audit logs track backup configuration changes.
    â€¢ LTR aligns with data retention compliance frameworks.
    â€¢ Encryption via TDE with optional CMK from Azure Key Vault.
    â€¢ Use Azure Monitor or Log Analytics for alerts/metrics.

14. Best Practices
    â€¢ Choose redundancy based on SLA and DR requirements
    â€¢ Match backup frequency to RPO/RTO goals
    â€¢ Enable LTR for compliance
    â€¢ Use CMK + TDE if customer control is needed
    â€¢ Regularly review:
        â—‹ PITR/LTR retention
        â—‹ Deleted DBs window
        â—‹ SQL Server security settings
 
## Restoring SQL Using the Portal 


1. Importance of Restore
    â€¢ Ensures business continuity by recovering from:
        â—‹ Accidental deletion
        â—‹ Data corruption
        â—‹ Malware or ransomware
    â€¢ Built-in Azure SQL Database backups support point-in-time restore (PITR) and long-term retention (LTR)

2. Entry Point
    â€¢ Go to Azure Portal > SQL Databases
    â€¢ Select the SQL database you want to restore

3. Understand Backup Scope
    â€¢ Backups are configured and managed at the SQL Server level, not the individual database level
    â€¢ In the database blade, options like Compute + storage show the storage redundancy level but not full backup management

4. Backup Redundancy Configuration
    â€¢ Navigate to: SQL Database > Settings > Compute + Storage
    â€¢ Backup storage redundancy options:
        â—‹ Locally-redundant (LRS)
        â—‹ Zone-redundant (ZRS)
        â—‹ Geo-redundant (GRS) (Default)
    â€¢ GRS replicates backup blobs to a paired Azure region

5. Geo-Replication vs Backup
    â€¢ Geo-replica: Real-time sync replica for HA/disaster recovery
        â—‹ Access via Replicas blade
        â—‹ Use "Create replica" for regional failover
    â€¢ Backups: Point-in-time snapshots for true recovery
        â—‹ Available even if the primary DB is deleted

6. Navigate to Server-Level Backup
    â€¢ In database Overview blade: click Server name link
    â€¢ Under SQL Server > Data Management > Backups:
        â—‹ Tab: Available backups
        â—‹ Toggle: Active / Deleted Databases
        â—‹ Action column: Click Restore

7. Restore Process
    â€¢ Restore type: Choose between:
        â—‹ Point-in-time restore (PITR): Select timestamp
        â—‹ Long-term retention (LTR): Restore from weekly/monthly/yearly backups (if configured)
    â€¢ Restore wizard fields:
        â—‹ New database name auto-generated with date-time suffix
        â—‹ Server, Elastic Pool, Compute + Storage, Backup redundancy options
        â—‹ Click Review + Create, then Create

8. Post-Restore Steps
    â€¢ Monitor: Notification bell shows deployment progress
    â€¢ New DB appears in SQL Databases list
    â€¢ Itâ€™s a fully separate database instance

9. Query the Restored DB
    â€¢ Use Query Editor (Preview)
        â—‹ Login using SQL Authentication or AAD
        â—‹ Error: "Public network access disabled" if networking isnâ€™t configured
Fix:
    â€¢ Navigate to: SQL Server > Security > Networking
        â—‹ Enable Public access or
        â—‹ Add client IP / VNet firewall rule
    â€¢ Retry Query Editor to validate access
        â—‹ Example: Expand Tables, run SELECT TOP 1000 * FROM Customers

10. Security Considerations
    â€¢ Backup/restore process inherits:
        â—‹ RBAC permissions
        â—‹ TDE encryption (Transparent Data Encryption is ON by default)
    â€¢ Ensure firewall/network settings align with restored environment
    â€¢ Use Azure Monitor and Log Analytics to audit access and activity

11. Summary
    â€¢ Restoring SQL via Azure Portal is straightforward but managed at the server level
    â€¢ Supports compliance and operational recovery needs
    â€¢ Test your backup & restore process regularly to meet RPO/RTO requirements
    â€¢ Reinforce with retention policies, firewalls, and key vault encryption

AZ-500 Tips:
    â€¢ Know where SQL backups are configured (Server > Data Management)
    â€¢ Understand PITR vs LTR options
    â€¢ Backup storage redundancy settings
    â€¢ Networking/firewall requirements for restored DB access
    â€¢ Role of TDE, RBAC, and Azure Backup Server for SQL in VMs/on-prem

##  Enabling Storage Account Replication


I. Topic Overview
    â€¢ Title: Enabling Storage Account Replication
    â€¢ Presenter: Dan Lachance
    â€¢ Purpose: Achieve high availability for Azure storage accounts using replication

II. Replication Concept in Azure
    â€¢ Replication = Copying data to a secondary region
    â€¢ Known as geo-redundancy or geo-replication
    â€¢ Azure uses asynchronous replication:
        â—‹ Write completes on primary first
        â—‹ Then syncs to secondary (not simultaneous)

III. Navigating to Storage Accounts in Portal
    â€¢ Azure Portal â†’ Storage accounts
    â€¢ View includes:
        â—‹ Recent and Favorite tabs
        â—‹ Columns: Name, Type, Last Viewed

IV. Creating a New Storage Account with Replication
A. Click "Create" on Storage Accounts page
B. Configure Basics
    â€¢ Storage account name
    â€¢ Region
    â€¢ Performance tier
    â€¢ Redundancy (default: GRS)
C. Redundancy Dropdown Options
    â€¢ LRS: Local only, cheapest, no regional protection
    â€¢ ZRS: Across zones, protects against datacenter failures
    â€¢ GRS: Geo-redundant, secondary region added
    â€¢ GZRS: Combines ZRS and GRS for max durability
D. Default: GRS with Read Access
    â€¢ Checkbox auto-enabled for read-access in case of regional unavailability
E. Cancel Creation for Demo Purposes
    â€¢ Presenter instead opens existing storage account eastyhz1

V. Enabling Replication on Existing Storage Account
A. Open eastyhz1 â†’ Data Management â†’ Redundancy
B. Current Setup
    â€¢ Set to LRS
    â€¢ Map shows Primary: East US
    â€¢ No secondary region assigned
C. Change to GRS
    â€¢ Select from dropdown
    â€¢ Secondary region auto-assigned (e.g., West US)
    â€¢ Click Save
D. Post-Configuration
    â€¢ Secondary (West US) appears as Available
    â€¢ Initial sync in progress
    â€¢ Duration depends on account contents (blobs, tables, queues, files)

VI. Prepare and Perform Failover
A. After Sync Completion
    â€¢ Button "Prepare for failover" becomes available
B. Click "Prepare for failover"
    â€¢ Warnings shown:
        1. Last sync time â€“ possible data loss
        2. After failover, account becomes LRS
        3. You can reconfigure to GRS again later
C. Confirm Failover
    â€¢ Type yes â†’ Click Failover
D. Failover Progress
    â€¢ East US = Primary
    â€¢ West US = Secondary
    â€¢ Now in progress â†’ West US becomes new primary

VII. Post-Failover Behavior
A. Redundancy View Changes
    â€¢ Only one location now shown (West US as LRS)
    â€¢ Geo-replication removed after failover
B. DNS/Endpoint Behavior
    â€¢ No changes for apps/users
    â€¢ DNS (FQDN) remains same (e.g., eastyhz1.blob.core.windows.net)
    â€¢ Now points to the new primary (West US)
C. View Endpoints in Settings
    â€¢ Endpoints show same names
    â€¢ Reference the new primary region

VIII. Conclusion
    â€¢ Replication is asynchronous
    â€¢ Failover:
        â—‹ Temporary conversion to LRS
        â—‹ Must manually re-enable GRS/GZRS if needed
    â€¢ No endpoint reconfiguration required
    â€¢ Supports disaster recovery and high availability in Azure
 
## Backing Up Azure Web Applications


I. Introduction
    â€¢ Purpose: Enable data availability by backing up Azure Web Apps (App Services)
    â€¢ Some apps contain static content (e.g., PDFs) that rarely change
    â€¢ Covers automatic vs custom backups, partial backups, deployment slots, and restore options

II. Accessing App Service in Azure Portal
    â€¢ Navigate to App Services
    â€¢ Select running app (e.g., samplenewandwonderfulapp)
    â€¢ View app properties and settings (Region, Status, Resource Group, App Service Plan)

III. Default Backup Behavior
    â€¢ Automatic backup every 1 hour
    â€¢ Requires no manual storage account config
    â€¢ No partial backup support in default mode
    â€¢ Backup page shows:
        â—‹ List of backups
        â—‹ Status (Succeeded/Failed)
        â—‹ Type (Automatic)
        â—‹ Restore link (to current or other deployment slots)

IV. Deployment Slots
    â€¢ Found under "Deployment" in app settings
    â€¢ Default slot: Production
    â€¢ Optional: Add more slots (e.g., staging/testing)
    â€¢ Slot usage during restore:
        â—‹ Restore to non-production slot to avoid downtime

V. App Service Tier Impacts Backup
    â€¢ Go to Scale up (App Service plan) to check pricing tier
    â€¢ Example: Standard S1
        â—‹ Basic/Free tiers: only production slot backup/restore allowed
        â—‹ Standard/Premium tiers: support multiple slots for backup/restore

VI. Configure Custom Backups
    â€¢ Go to Backups â†’ Click Configure custom backups
    â€¢ Custom backup steps:
        1. Select Storage account
        2. Create or choose Blob container (e.g., webappbackup)
        3. Set schedule:
            Â§ Example: Every 1 Day (can be hourly)
            Â§ Define start time, time zone
        4. Set retention:
            Â§ Default: 30 days
            Â§ 0 = indefinite (increases cost)
            Â§ Optional: "Keep at least one backup at all times"
        5. Click Next: Advanced
            Â§ If linked DB exists, option to back it up appears
            Â§ If no DBs, table is empty
        6. Click Configure

VII. Backup Now Option
    â€¢ Once custom backup is configured:
        â—‹ "Backup Now" button is enabled
        â—‹ Click to trigger on-demand backup
        â—‹ Shows Status: In Progress
        â—‹ After completion, shows Status: Succeeded

VIII. Creating a Partial Backup
    â€¢ Use Kudu Debug Console:
https://<appname>.scm.azurewebsites.net/DebugConsole
    â€¢ Navigate to: site/wwwroot/
    â€¢ Create a file: _backup.filter
    â€¢ Inside file: list of files/folders to exclude (e.g., docs/, static/)
    â€¢ Upload via drag-and-drop in console or FTP

IX. Restoring from Backup
    â€¢ Click Restore for desired backup
    â€¢ Source options:
        â—‹ Automatic backup
        â—‹ Custom backup
        â—‹ Storage (external Blob backup)
    â€¢ Destination options:
        â—‹ Existing deployment slot
        â—‹ Create new app
    â€¢ Advanced options:
        â—‹ Ignore conflicting domain names
        â—‹ Include database
    â€¢ Click Restore â†’ monitor via Notification bell

X. Key Points Summary
    â€¢ Default: hourly full backup; no config needed
    â€¢ Custom backups allow scheduling, filtering, retention
    â€¢ Only supported in Standard tier and above
    â€¢ _backup.filter enables partial backups
    â€¢ Restore supports production or staging slots, or new apps
    â€¢ DB backup and restore optional in advanced settings
 
## Backing Up Azure Files Shares


I. Introduction
    â€¢ Purpose: Demonstrate how to back up an Azure Files shared folder
    â€¢ Covers:
        â—‹ Creating file shares
        â—‹ Uploading content
        â—‹ Enabling snapshots
        â—‹ Enabling backup via Recovery Services Vault
        â—‹ Triggering and restoring backups

II. Initial Setup: Access Storage Account
A. Navigate to Storage Account
    â€¢ Azure Portal â†’ Storage accounts
    â€¢ Select an existing storage account (e.g., eastyhz1)
B. Go to File Shares
    â€¢ Left pane â†’ Data Storage â†’ File shares
    â€¢ View existing shares or create a new one

III. Create and Populate a File Share
A. Create New File Share
    â€¢ Click + File Share
    â€¢ Name: e.g., projects
    â€¢ Click Create
B. Add Directory and Files
    â€¢ Open file share â†’ Click Add Directory (e.g., current_year)
    â€¢ Click Browse â†’ Click Upload to upload files
C. Optional: Connect Locally
    â€¢ Click Connect (top of the page)
    â€¢ Instructions for Windows (map drive letter), Linux, macOS
    â€¢ Can be used by local backup software

IV. Create Snapshot (Point-in-Time Copy)
A. Snapshots for Manual Protection
    â€¢ Left pane â†’ Operations â†’ Snapshots
    â€¢ Click Add Snapshot (name: snapshot1)
    â€¢ Snapshot can be browsed or mounted via SMB

V. Configure Azure Backup for File Share
A. Go to Backup
    â€¢ Left pane under Operations â†’ Click Backup
B. Select/Create Recovery Services Vault
    â€¢ Choose existing or create new vault
    â€¢ Assign to a Resource Group
C. Choose or Edit Backup Policy
    â€¢ Default: Daily at 7:30 PM, 30 days retention
    â€¢ Optional changes:
        â—‹ Hourly/Daily frequency
        â—‹ Retain daily, weekly, monthly, yearly points
        â—‹ Timezone and retention sliders
D. Storage Account Lock
    â€¢ Enabled by default to prevent accidental deletion of the storage account during backup
E. Click Enable Backup
    â€¢ Triggers deployment (ConfigureProtection)

VI. Verify and Run Initial Backup
A. Access Recovery Services Vault
    â€¢ Go to Backup items under Protected items
    â€¢ Find Azure Storage (Azure Files)
B. Initial Status
    â€¢ File share (e.g., projects) shows as pending
    â€¢ Click the ellipsis (three dots) â†’ Select Backup now
C. Confirm Backup
    â€¢ Retain setting defaulted
    â€¢ Click OK
    â€¢ After completion, status shows Success with timestamp
D. Alternative View in File Share
    â€¢ Back in storage account â†’ File shares â†’ projects â†’ Backup tab
    â€¢ View:
        â—‹ Recovery vault
        â—‹ Last backup status
        â—‹ Jobs in the last 24 hours

VII. Monitor Backup Jobs
    â€¢ View Backup Jobs page
    â€¢ See status and history of operations

VIII. Restore File Share
A. Initiate Restore
    â€¢ In Recovery Vault â†’ Backup Items â†’ Click ellipsis â†’ Restore Share
B. Choose Restore Point
    â€¢ Pick a backup point (once completed)
C. Restore Destination Options
    1. Original Location
        â—‹ Conflict handling: Overwrite or Skip
    2. Alternate Location
        â—‹ Choose:
            Â§ Storage account
            Â§ File share
            Â§ Folder path
            Â§ Conflict handling
D. Click Restore
    â€¢ Monitored in Notification Bell

IX. Key Points Summary
    â€¢ Backing up Azure Files requires:
        â—‹ File Share in Storage Account
        â—‹ Recovery Services Vault
        â—‹ Backup policy
    â€¢ Snapshots offer manual protection
    â€¢ Backup jobs and restore options are visible in both storage account and vault
    â€¢ Restore supports overwrite, skip, or restore to alternate folder or storage account

 
## Managing Data Archiving and Rehydration


I. Introduction
    â€¢ Why archive? Legal, regulatory, or contractual reasons may require data retention even if not accessed frequently.
    â€¢ Blob storage tiers support cost-optimized retention:
        â—‹ Hot: Frequently accessed
        â—‹ Cool: Infrequently accessed
        â—‹ Archive: Rarely accessed, cheapest, offline until rehydrated

II. Accessing Blob Containers in the Portal
A. Navigate to Storage Accounts â†’ Open storage account (e.g., eastyhz1)
B. Go to Containers under Data Storage
C. Open a container (e.g., budgets)
    â€¢ Files listed with info like Name, Modified, Access tier, etc.

III. Manually Changing Blob Access Tier
A. Change Tier Button Behavior
    â€¢ Grayed out if:
        â—‹ No file selected
        â—‹ Multiple blobs selected
    â€¢ Available when one blob is selected
B. Change Tier Flow
    â€¢ Select blob â†’ Click Change tier
    â€¢ Choose from:
        â—‹ Hot (default)
        â—‹ Cool
        â—‹ Archive â† chosen in demo
    â€¢ Warning: Archive makes blob inaccessible until rehydrated
    â€¢ Cost impact if rehydrated before 180 days
C. Result
    â€¢ Blob marked Archive
    â€¢ Appears as a stub (unavailable for download/edit)

IV. Using PowerShell (Cloud Shell) to View Blob Tiers
A. Commands Used:
$acc = Get-AzStorageAccount -Name "eastyhz1" -ResourceGroupName "App1"
Get-AzStorageContainer -Context $acc.Context -Name budgets | Get-AzStorageBlob
    â€¢ Displays blobs and current access tiers (e.g., Archive)

V. Rehydrating an Archived Blob
A. Open blob â†’ Click Change tier
B. Select new tier:
    â€¢ Hot or Cool (Cool used in demo)
    â€¢ Choose Rehydrate Priority:
        â—‹ Standard (default)
        â—‹ High (faster but more expensive, for emergencies)
C. Click Save
    â€¢ Status: Rehydrate Pending
    â€¢ Archive status updates once complete
    â€¢ Access tier changes to Cool (or Hot)

VI. Post-Rehydration Behavior
    â€¢ Blob becomes accessible again
    â€¢ Buttons like Download reappear
    â€¢ Tier can be changed again as needed

VII. Automating Tier Changes with Lifecycle Management
A. Navigate to storage account â†’ Data management â†’ Lifecycle management
B. Click Add a rule
C. Rule Options:
    â€¢ Scope: All blobs or filtered subset
    â€¢ Conditions:
        â—‹ If blob was last modified or created > X days ago
    â€¢ Actions:
        â—‹ Move to Cool
        â—‹ Move to Archive
        â—‹ Delete blob
        â—‹ Option: Skip blobs rehydrated in last 7 days
D. Use case:
    â€¢ Example: Archive blobs not modified in the last 90 days

VIII. Key Points Summary
    â€¢ Hot/Cool/Archive tiers support storage cost optimization
    â€¢ Archive is offline; requires rehydration
    â€¢ Manual and automated tiering (via lifecycle rules) supported
    â€¢ Rehydration may take minutes to hours
    â€¢ PowerShell can be used for tier inspection
    â€¢ Lifecycle rules allow automated archival/deletion based on age or access

# Review Lab type shit

## Managing User Permissions to Azure Resources

1. Managing User Permissions to Azure Resources
Objective:
    â€¢ Implement least-privilege access by grouping users based on attributes (like city = Toronto) and assigning appropriate roles at the resource level.
    â€¢ Ensure dynamic scaling of permissions as users meet group criteria.
Why It Matters:
    â€¢ Prevents over-provisioning of access
    â€¢ Supports dynamic environments (e.g., hiring in Toronto auto-adds users to the right group with right access)
    â€¢ Complies with Zero Trust principle of role-based access controls (RBAC)
Portal Steps:
    1. Create Dynamic Group:
        â—‹ Azure Active Directory > Groups > + New group
        â—‹ Group type: Security
        â—‹ Group name: Toronto_Users
        â—‹ Membership type: Dynamic User
        â—‹ Add owner: Abu Adachi
        â—‹ Add dynamic rule: Property = city, Operator = equals, Value = Toronto
    2. Add User:
        â—‹ Azure AD > Users > + New user
        â—‹ Set Name: Julio Chavez, UPN: jchavez@yourdomain.com
        â—‹ After creation, open profile â†’ Edit properties â†’ Set City = Toronto
    3. Assign Role to Group:
        â—‹ Resource Group > App1 > IAM > Add Role Assignment
        â—‹ Role: Storage Blob Data Reader
        â—‹ Assign to: User, group, or service principal â†’ Select Toronto_Users
PowerShell:
# Create user
New-AzADUser -DisplayName "Julio Chavez" -UserPrincipalName "jchavez@yourdomain.com" \
  -MailNickname "jchavez" -PasswordProfile @{Password="Password123!"; ForceChangePasswordNextLogin=$true} \
  -AccountEnabled $true -UsageLocation "CA"
# Set user city
Update-MgUser -UserId "jchavez@yourdomain.com" -City "Toronto"
# Create dynamic group
$rule = '(user.city -eq "Toronto")'
New-MgGroup -DisplayName "Toronto_Users" -MailEnabled:$false -MailNickname "torontousers" \
  -SecurityEnabled:$true -GroupTypes @("DynamicMembership") \
  -MembershipRule $rule -MembershipRuleProcessingState "On"
# Assign role
$group = Get-AzADGroup -DisplayName "Toronto_Users"
New-AzRoleAssignment -ObjectId $group.Id -RoleDefinitionName "Storage Blob Data Reader" -ResourceGroupName "App1"
Azure CLI:
az ad user create --display-name "Julio Chavez" \
  --user-principal-name jchavez@yourdomain.com \
  --password "Password123!" --force-change-password-next-login true
az rest --method patch --uri "https://graph.microsoft.com/v1.0/users/jchavez@yourdomain.com" \
  --headers "Content-Type=application/json" \
  --body '{"city":"Toronto"}'
az ad group create --display-name "Toronto_Users" --mail-nickname "torontousers"
az rest --method patch --uri "https://graph.microsoft.com/v1.0/groups/<groupId>" \
  --headers "Content-Type=application/json" \
  --body '{ "groupTypes":["DynamicMembership"], "membershipRuleProcessingState":"On", "membershipRule":"(user.city -eq \"Toronto\")" }'
az role assignment create --assignee <group-object-id> --role "Storage Blob Data Reader" --resource-group App1

## Defining Custom RBAC Roles


    â€¢ Defining Custom RBAC Roles
Objective:
Create and assign a custom RBAC role that provides:
        â—‹ Full VM management permissions
        â—‹ Read-only access to Blob storage
        â—‹ Scope: Resource group App1


Security Context / Why We Do This:
        â—‹ Custom roles offer granular control when built-in roles donâ€™t match the exact operational needs.
        â—‹ Helps meet least privilege and compliance mandates.
    â€¢ Limits lateral movement potential by restricting scope to App1 only.

Key Concepts from Video:
    â€¢ Combine multiple actions into a single role: VM management + Blob read
    â€¢ Ensure the assignable scope is set to avoid global role misuse
    â€¢ Roles must follow JSON schema (or portal equivalent)

Portal Steps:
    1. Go to Subscriptions > Choose subscription > Access control (IAM) > + Add > Add custom role
        â—‹ Name: Custom VM Management
        â—‹ Start from scratch
        â—‹ Permissions:
            Â§ Add: Microsoft.Compute/*
            Â§ Add: Microsoft.Storage/storageAccounts/blobServices/containers/blobs/read
        â—‹ Assignable scope: /subscriptions/<subId>/resourceGroups/App1
        â—‹ Click Review + create
    2. Assign Role to User
        â—‹ Go to Resource groups > App1 > Access control (IAM)
        â—‹ Click + Add > Add role assignment
        â—‹ Filter by: Custom VM Management
        â—‹ Assign to: Abu

PowerShell:
# Create custom role definition
$customRole = @{
  Name = "Custom VM Management"
  Id = (New-Guid).Guid
  IsCustom = $true
  Description = "VM admin and blob read"
  Actions = @(
    "Microsoft.Compute/*",
    "Microsoft.Storage/storageAccounts/blobServices/containers/blobs/read"
  )
  NotActions = @()
  DataActions = @()
  NotDataActions = @()
  AssignableScopes = @("/subscriptions/<subId>/resourceGroups/App1")
}
$roleJson = $customRole | ConvertTo-Json -Depth 10
$rolePath = "./customRole.json"
$roleJson | Out-File $rolePath
New-AzRoleDefinition -InputFile $rolePath
# Assign role
$user = Get-AzADUser -DisplayName "Abu"
New-AzRoleAssignment -ObjectId $user.Id -RoleDefinitionName "Custom VM Management" -Scope "/subscriptions/<subId>/resourceGroups/App1"

Azure CLI:
# Create custom role definition
cat <<EOF > custom-role.json
{
  "Name": "Custom VM Management",
  "IsCustom": true,
  "Description": "VM admin and blob read",
  "Actions": [
    "Microsoft.Compute/*",
    "Microsoft.Storage/storageAccounts/blobServices/containers/blobs/read"
  ],
  "AssignableScopes": ["/subscriptions/<subId>/resourceGroups/App1"]
}
EOF
az role definition create --role-definition custom-role.json
# Assign role
userId=$(az ad user show --id abu@yourdomain.com --query id -o tsv)
az role assignment create \
  --assignee $userId \
  --role "Custom VM Management" \
  --scope "/subscriptions/<subId>/resourceGroups/App1"

##  Configuring Conditional Access Policies


3. Configuring Conditional Access Policies
Objective:
    â€¢ Protect cloud applications using identity-based conditions and contextual signals
    â€¢ Create policy for app "Mobile Xpense" requiring MFA, Android platform, from trusted subnet
Why It Matters:
    â€¢ Prevents unauthorized access from unknown devices/locations
    â€¢ Combats credential theft via phishing by enforcing MFA
    â€¢ Meets compliance requirements (e.g., location-aware access)
Security Context / Why We Do This:
    â€¢ Conditional Access enforces Zero Trust principles: never trust, always verify.
    â€¢ Ensures only compliant devices from trusted networks can access sensitive apps.
    â€¢ Combines user context, device state, and network location to control access.
    
Portal Steps:
    1. Create named location:
        â—‹ Azure AD > Security > Named Locations > Add IP range
        â—‹ Name: Headquarters Europe
        â—‹ IP Range: 192.168.1.0/24 â†’ Mark as trusted
    2. Create Conditional Access Policy:
        â—‹ Azure AD > Security > Conditional Access > New Policy
        â—‹ Name: Allow Access to Mobile Xpense
        â—‹ Assign to: All Users
        â—‹ Cloud App: Mobile Xpense
        â—‹ Conditions:
            Â§ Device Platform: Android
            Â§ Locations: Include Headquarters Europe
        â—‹ Access Control: Grant access â†’ Require MFA
        â—‹ Enable Policy: On
PowerShell:
Connect-MgGraph -Scopes "Policy.ReadWrite.ConditionalAccess"
New-MgConditionalAccessNamedLocation -DisplayName "Headquarters Europe" `
  -IpRange @{Ranges="192.168.1.0/24"; IsTrusted=$true}
New-MgConditionalAccessPolicy -DisplayName "Allow Access to Mobile Xpense" `
  -State "enabled" `
  -Conditions @{
      Users = @{Include = @("All")}
      Platforms = @{Include = @("android")}
      Locations = @{Include = @("<location-id>")}
    } `
  -GrantControls @{BuiltInControls = @("mfa")} `
  -Applications @{IncludeApplications = @("<mobile-xpense-app-id>")}
Azure CLI:
    âŒ Not available in az CLI directly. Use Microsoft Graph CLI or REST API.

## Assigning Permissions to Azure VMs

4. Assigning Permissions to Azure VMs
Objective:
    â€¢ Configure a system-assigned managed identity on a VM (WinSrv2019-2)
    â€¢ Grant it the Storage Blob Data Reader role
    â€¢ Allow the VM to securely access storage blobs in the App1 resource group

Security Context / Why We Do This:
    â€¢ Avoids use of hard-coded secrets or credentials in scripts and apps
    â€¢ Enforces least privilege via role-based access control (RBAC)
    â€¢ Enables secure service-to-service authentication within Azure
    â€¢ Managed identities are automatically rotated and protected by Azure AD

Key Concepts from Video:
    â€¢ System-assigned identity is bound to the lifecycle of the VM. When the VM is deleted, the identity is automatically removed.
    â€¢ Assigning roles to this identity enables secure access to Azure resources, like storage accounts, without embedding credentials.
    â€¢ In this scenario, the identity will be used to access blobs, which can contain app configurations, logs, or other shared data.

Portal Steps:
    1. Enable System-Assigned Identity
        â—‹ Go to Virtual Machines > Select WinSrv2019-2
        â—‹ Click Identity under the Settings section
        â—‹ Set System-assigned status to On and click Save
    2. Grant Storage Permissions
        â—‹ Navigate to the Storage Account where blob data resides
        â—‹ Go to Access Control (IAM) â†’ + Add â†’ Add role assignment
        â—‹ In Role, search for and select Storage Blob Data Reader
        â—‹ In Assign access to, choose Managed identity
        â—‹ Click Select members â†’ find and select WinSrv2019-2
        â—‹ Click Review + assign

PowerShell:

powershell
CopyEdit
# Enable system-assigned identity
$vm = Get-AzVM -Name "WinSrv2019-2" -ResourceGroupName "App1"
$vm.Identity.Type = 'SystemAssigned'
Update-AzVM -VM $vm -ResourceGroupName "App1"
# Retrieve the identity's principal ID
$identity = (Get-AzVM -ResourceGroupName "App1" -Name "WinSrv2019-2").Identity.PrincipalId
# Assign Storage Blob Data Reader role
New-AzRoleAssignment `
  -ObjectId $identity `
  -RoleDefinitionName "Storage Blob Data Reader" `
  -Scope "/subscriptions/<subId>/resourceGroups/App1/providers/Microsoft.Storage/storageAccounts/<storageAccountName>"

Azure CLI:

bash
CopyEdit
# Enable system-assigned identity
az vm identity assign \
  --name WinSrv2019-2 \
  --resource-group App1
# Retrieve identity principal ID
principalId=$(az vm show -g App1 -n WinSrv2019-2 --query identity.principalId -o tsv)
# Assign role to identity
az role assignment create \
  --assignee $principalId \
  --role "Storage Blob Data Reader" \
  --scope "/subscriptions/<subId>/resourceGroups/App1/providers/Microsoft.Storage/storageAccounts/<storageAccountName>"


## Hardening Azure SQL Managed Instance



5. Hardening Azure SQL Managed Instance
Objective:
    â€¢ Apply foundational security controls to protect an Azure SQL Database:
        â—‹ Enable Transparent Data Encryption (TDE)
        â—‹ Turn on Microsoft Defender for Cloud
        â—‹ Configure daily backups with retention
        â—‹ Apply Dynamic Data Masking (DDM)
        â—‹ Restrict access via SQL firewall rules

Security Context / Why We Do This:
    â€¢ TDE encrypts data at rest, protecting against unauthorized access to underlying storage
    â€¢ Defender for SQL offers threat detection and vulnerability scanning
    â€¢ Firewall rules control who can connect to the SQL instance
    â€¢ DDM obfuscates sensitive data in query results without changing the underlying database
    â€¢ Backup policies ensure recoverability and regulatory compliance (e.g., HIPAA, ISO)

Key Concepts from Video:
    â€¢ SQL is often a critical assetâ€”customer, financial, or health data is often stored here
    â€¢ Misconfiguration can lead to data leaks, especially when accessed from public IPs
    â€¢ Best practice: Combine platform security (TDE, firewall) with identity controls and masking
    â€¢ Defender can send alerts to Microsoft Defender for Cloud, which provides recommendations

Portal Steps:
    1. Enable Transparent Data Encryption (TDE):
        â—‹ Go to SQL databases > Select your DB (e.g., db1)
        â—‹ Under Security, click Transparent Data Encryption
        â—‹ Set to ON â†’ Click Save
    2. Enable Microsoft Defender for SQL:
        â—‹ Go to Microsoft Defender for Cloud > Environment settings
        â—‹ Select the subscription > SQL servers
        â—‹ Enable Microsoft Defender for SQL for your server (e.g., sqlserver01)
    3. Configure Backup Retention Policy:
        â—‹ Navigate to SQL Server > sqlserver01 > Backups
        â—‹ Set Retention policy â†’ Choose 24 hours (or as needed)
        â—‹ Save settings
    4. Apply Dynamic Data Masking (DDM):
        â—‹ Go to SQL databases > Select db1 > Security > Dynamic Data Masking
        â—‹ Click + Add masking rule
            Â§ Choose sensitive column (e.g., SSN, creditCard)
            Â§ Set masking format (e.g., default, custom string)
        â—‹ Click Add
    5. Restrict IP Access via Firewall Rules:
        â—‹ Go to SQL Server > sqlserver01 > Networking
        â—‹ Click + Add client IP or define range (e.g., 192.168.1.0/24)
        â—‹ Click Save

PowerShell:
# Enable Transparent Data Encryption
Set-AzSqlDatabaseTransparentDataEncryption `
  -ResourceGroupName "App1" `
  -ServerName "sqlserver01" `
  -DatabaseName "db1" `
  -State "Enabled"
# Enable Microsoft Defender (Threat Detection Policy)
Set-AzSqlServerThreatDetectionPolicy `
  -ResourceGroupName "App1" `
  -ServerName "sqlserver01" `
  -State Enabled
# Add firewall rule
New-AzSqlServerFirewallRule `
  -ResourceGroupName "App1" `
  -ServerName "sqlserver01" `
  -FirewallRuleName "AllowMyIP" `
  -StartIpAddress "192.168.1.1" `
  -EndIpAddress "192.168.1.1"

Azure CLI:
# Enable TDE
az sql db tde set \
  --resource-group App1 \
  --server sqlserver01 \
  --name db1 \
  --status Enabled
# Enable Defender for SQL (threat detection)
az sql server threat-policy update \
  --resource-group App1 \
  --server sqlserver01 \
  --state Enabled
# Add firewall rule
az sql server firewall-rule create \
  --resource-group App1 \
  --server sqlserver01 \
  --name AllowMyIP \
  --start-ip-address 192.168.1.1 \
  --end-ip-address 192.168.1.1
> 

##  Encrypting Azure VM Disks


7. Configuring Time-Limited Restricted Storage Account Access
Objective:
    â€¢ Use a Shared Access Signature (SAS) token to grant temporary, IP-restricted access
    â€¢ Scope: allow read and list operations on blob containers
    â€¢ Enforce access only from a specific IP range and time window

Security Context / Why We Do This:
    â€¢ A SAS token enables granular, time-bound access to storage resources without giving full credentials
    â€¢ This is ideal for:
        â—‹ Third-party developers
        â—‹ Temporary access during migration
        â—‹ Restricting downloads/uploads to a known IP or time
    â€¢ Prevents abuse of long-lived credentials by limiting what actions are allowed, from where, and for how long
    â€¢ SAS tokens can be revoked by regenerating storage account keys

Key Concepts from Video:
    â€¢ SAS tokens support service-level permissions (Blob, File, Queue, Table)
    â€¢ You must define what, who, how long, and from where when generating a token
    â€¢ SAS URLs can be embedded in scripts, apps, or sent to external parties
    â€¢ Tip: Store SAS usage logs using Storage analytics

Portal Steps:
    1. Navigate to Storage Account
        â—‹ Go to Storage Accounts > Choose your storage (e.g., mystorageeast)
        â—‹ Under Settings, click Shared access signature
    2. Configure SAS Settings
        â—‹ Services: Select only Blob
        â—‹ Resource types: Select Container and Object
        â—‹ Permissions: Check Read and List
        â—‹ Start time: Set current date/time
        â—‹ Expiry time: Set a future date/time (e.g., 2 hours later)
        â—‹ Allowed IP addresses: Enter range (e.g., 192.168.100.0/24)
        â—‹ Allowed protocols: HTTPS only
        â—‹ Click Generate SAS and connection string
    3. Copy and Share SAS URL
        â—‹ Copy the generated SAS token or full blob URL with query string
        â—‹ Paste into browser or use in PowerShell/CLI to test

PowerShell:
# Connect to storage account context
$ctx = New-AzStorageContext -StorageAccountName "mystorageeast" -StorageAccountKey "<storageKey>"
# Generate SAS for container
New-AzStorageContainerSASToken `
  -Name "backups" `
  -Context $ctx `
  -Permission rl `
  -StartTime (Get-Date) `
  -ExpiryTime (Get-Date).AddHours(2) `
  -Protocol HttpsOnly `
  -IPAddressOrRange "192.168.100.0/24" `
  -FullUri

Azure CLI:
# Generate SAS token for container
az storage container generate-sas \
  --account-name mystorageeast \
  --name backups \
  --permissions rl \
  --expiry "$(date -u -d '2 hours' +%Y-%m-%dT%H:%MZ)" \
  --ip "192.168.100.0/24" \
  --https-only \
  --auth-mode key \
  --output tsv
    ğŸ” Pro tip: Use the resulting SAS token in this format:
    https://mystorageeast.blob.core.windows.net/backups?<sas_token_here>

## Configuring Time-limited Restricted Storage Account Access


7. Configuring Time-Limited Restricted Storage Account Access
Objective:
    â€¢ Use a Shared Access Signature (SAS) token to grant temporary, IP-restricted access
    â€¢ Scope: allow read and list operations on blob containers
    â€¢ Enforce access only from a specific IP range and time window

Security Context / Why We Do This:
    â€¢ A SAS token enables granular, time-bound access to storage resources without giving full credentials
    â€¢ This is ideal for:
        â—‹ Third-party developers
        â—‹ Temporary access during migration
        â—‹ Restricting downloads/uploads to a known IP or time
    â€¢ Prevents abuse of long-lived credentials by limiting what actions are allowed, from where, and for how long
    â€¢ SAS tokens can be revoked by regenerating storage account keys

Key Concepts from Video:
    â€¢ SAS tokens support service-level permissions (Blob, File, Queue, Table)
    â€¢ You must define what, who, how long, and from where when generating a token
    â€¢ SAS URLs can be embedded in scripts, apps, or sent to external parties
    â€¢ Tip: Store SAS usage logs using Storage analytics

Portal Steps:
    1. Navigate to Storage Account
        â—‹ Go to Storage Accounts > Choose your storage (e.g., mystorageeast)
        â—‹ Under Settings, click Shared access signature
    2. Configure SAS Settings
        â—‹ Services: Select only Blob
        â—‹ Resource types: Select Container and Object
        â—‹ Permissions: Check Read and List
        â—‹ Start time: Set current date/time
        â—‹ Expiry time: Set a future date/time (e.g., 2 hours later)
        â—‹ Allowed IP addresses: Enter range (e.g., 192.168.100.0/24)
        â—‹ Allowed protocols: HTTPS only
        â—‹ Click Generate SAS and connection string
    3. Copy and Share SAS URL
        â—‹ Copy the generated SAS token or full blob URL with query string
        â—‹ Paste into browser or use in PowerShell/CLI to test

PowerShell:
# Connect to storage account context
$ctx = New-AzStorageContext -StorageAccountName "mystorageeast" -StorageAccountKey "<storageKey>"
# Generate SAS for container
New-AzStorageContainerSASToken `
  -Name "backups" `
  -Context $ctx `
  -Permission rl `
  -StartTime (Get-Date) `
  -ExpiryTime (Get-Date).AddHours(2) `
  -Protocol HttpsOnly `
  -IPAddressOrRange "192.168.100.0/24" `
  -FullUri

Azure CLI:
# Generate SAS token for container
az storage container generate-sas \
  --account-name mystorageeast \
  --name backups \
  --permissions rl \
  --expiry "$(date -u -d '2 hours' +%Y-%m-%dT%H:%MZ)" \
  --ip "192.168.100.0/24" \
  --https-only \
  --auth-mode key \
  --output tsv
    ğŸ” Pro tip: Use the resulting SAS token in this format:
    https://mystorageeast.blob.core.windows.net/backups?<sas_token_here>


## Creating a Compliant Cloud Sandbox


8. Creating a Compliant Cloud Sandbox
Objective:
    â€¢ Use Azure Blueprints to deploy a secure, policy-compliant sandbox:
        â—‹ Create a resource group named Sandbox
        â—‹ Assign Contributor access to a group named App1
        â—‹ Enforce policies: SQL auditing and allowed locations (East US only)

Security Context / Why We Do This:
    â€¢ Cloud sandboxes are often used for testing, training, or staging with minimal risk to production
    â€¢ Applying RBAC + Azure Policy ensures:
        â—‹ No unauthorized role sprawl
        â—‹ Resources stay in approved locations
        â—‹ Compliance mandates (like CIS, FedRAMP) are enforced automatically
    â€¢ Blueprints support repeatable deployments, version control, and auditable governance

Key Concepts from Video:
    â€¢ Blueprints are higher-level governance tools that combine:
        â—‹ Resource templates
        â—‹ Role assignments
        â—‹ Policy assignments
    â€¢ They ensure that each new environment is built securely by design
    â€¢ Blueprint assignments are tracked and can be locked from tampering

Portal Steps:
    1. Create a Blueprint
        â—‹ Go to Azure Blueprints > Create > Start with a Blank blueprint
        â—‹ Name: CompliantSandbox
        â—‹ Assign to a Management Group or Subscription
    2. Add Artifacts to the Blueprint
        â—‹ Click Add artifact â†’ Select type: Resource group
            Â§ Name: Sandbox, Location: East US
        â—‹ Add Role Assignment:
            Â§ Role: Contributor
            Â§ Principal: Azure AD Group App1
        â—‹ Add Policy Assignment:
            Â§ Policy: Audit SQL server configurations
            Â§ Policy: Allowed Locations â†’ Set to only allow East US
    3. Publish and Assign the Blueprint
        â—‹ Click Publish blueprint â†’ Add version (e.g., v1.0)
        â—‹ Click Assign blueprint
            Â§ Select Subscription
            Â§ Lock Assignment: Read Only or Do Not Lock
            Â§ Enter parameter values (e.g., location = East US)
            Â§ Click Assign

PowerShell:
    âš ï¸ PowerShell support for Blueprints requires the Az.Blueprint module (may need manual install)
# Install module if needed
Install-Module -Name Az.Blueprint
# Define blueprint
New-AzBlueprint -Name "CompliantSandbox" -SubscriptionId <subId> -DisplayName "Compliant Cloud Sandbox"
# Add resource group artifact
New-AzBlueprintArtifact `
  -BlueprintName "CompliantSandbox" `
  -ArtifactName "SandboxRG" `
  -ResourceGroupArtifact `
  -DisplayName "Sandbox RG" `
  -ResourceGroupName "Sandbox" `
  -Location "East US"
# Assign contributor role to group
New-AzBlueprintArtifact `
  -BlueprintName "CompliantSandbox" `
  -ArtifactName "ContributorRole" `
  -RoleAssignmentArtifact `
  -DisplayName "Contributor Access" `
  -PrincipalId <GroupObjectId> `
  -RoleDefinitionId "/subscriptions/<subId>/providers/Microsoft.Authorization/roleDefinitions/<ContributorRoleId>"
# Assign the blueprint
Set-AzBlueprintAssignment -Name "CompliantSandbox" -SubscriptionId <subId>

Azure CLI:
    âŒ Azure CLI does not support Blueprints natively.
    âœ… Use ARM templates or REST API for automation.
    âš™ï¸ Workaround CLI path: deploy equivalent with az deployment sub create and policy assignments:
# Assign 'Allowed Locations' policy manually
az policy assignment create \
  --name "LimitLocations" \
  --policy "b24988ac-6180-42a0-ab88-20f7382dd24c" \
  --params '{ "listOfAllowedLocations": { "value": [ "eastus" ] } }' \
  --scope "/subscriptions/<subId>"
# Assign 'Audit SQL configurations'
az policy assignment create \
  --name "AuditSQL" \
  --policy "0e3a6b26-1e2e-4b6b-89f3-4b61b6359c79" \
  --scope "/subscriptions/<subId>"


## Generating Key Vault Secrets

9. Generating Key Vault Secrets
Objective:
In Key Vault KVCentral, you will:
    â€¢ Store a database connection string as a secret
    â€¢ Generate a self-signed certificate named WebApp1 with subject CN=www.webapp1.local
    â€¢ Create an encryption key (Key1) using RSA 2048-bit key

Security Context / Why We Do This:
    â€¢ Key Vault is a centralized tool to securely manage:
        â—‹ Secrets: passwords, connection strings, API keys
        â—‹ Keys: encryption keys for services like SQL TDE, VM disk encryption
        â—‹ Certificates: SSL/TLS for websites and apps
    â€¢ Avoids hardcoding secrets in source code or storing them in unsecured files
    â€¢ Supports RBAC and access policies, plus full audit logging and integration with Managed Identity

Key Concepts from Video:
    â€¢ Never store secrets in plain text or local config files â€” always use Key Vault
    â€¢ Certificates can be self-signed or issued by a CA (e.g., DigiCert, Sectigo)
    â€¢ Keys can be used in services like Disk Encryption, Azure SQL, or Custom Apps
    â€¢ Access is tightly controlled with role-based access or legacy access policies

Portal Steps:
    1. Create a Key Vault (if needed):
        â—‹ Azure Portal > Key Vaults > + Create
        â—‹ Name: KVCentral
        â—‹ Resource group: App1
        â—‹ Region: Central US
        â—‹ Pricing tier: Standard
        â—‹ Enable soft-delete and RBAC permissions model
    2. Add a Secret:
        â—‹ Navigate to KVCentral > Secrets > + Generate/Import
        â—‹ Upload method: Manual
        â—‹ Name: DBConnectionString1
        â—‹ Value: Server=sqlserver01;Database=appdb;User Id=admin;Password=SecureP@ssw0rd
        â—‹ Click Create
    3. Create a Key:
        â—‹ Navigate to KVCentral > Keys > + Generate
        â—‹ Name: Key1
        â—‹ Key type: RSA
        â—‹ RSA key size: 2048
        â—‹ Click Create
    4. Create a Certificate:
        â—‹ Navigate to KVCentral > Certificates > + Generate/Import
        â—‹ Method: Generate
        â—‹ Name: WebApp1
        â—‹ Certificate Type: Self-signed
        â—‹ Subject: CN=www.webapp1.local
        â—‹ Validity: 12 months (default)
        â—‹ Click Create

PowerShell:
# Create a Key Vault (if needed)
New-AzKeyVault -Name "KVCentral" -ResourceGroupName "App1" -Location "Central US"
# Add a secret
Set-AzKeyVaultSecret `
  -VaultName "KVCentral" `
  -Name "DBConnectionString1" `
  -SecretValue (ConvertTo-SecureString "Server=sqlserver01;Database=appdb;User Id=admin;Password=SecureP@ssw0rd" -AsPlainText -Force)
# Create a software-protected key
Add-AzKeyVaultKey `
  -VaultName "KVCentral" `
  -Name "Key1" `
  -Destination "Software"
# Create a self-signed certificate
$policy = Get-AzKeyVaultCertificatePolicy -SubjectName "CN=www.webapp1.local"
Add-AzKeyVaultCertificate `
  -VaultName "KVCentral" `
  -Name "WebApp1" `
  -Policy $policy

Azure CLI:
# Create Key Vault
az keyvault create \
  --name KVCentral \
  --resource-group App1 \
  --location "centralus"
# Add secret
az keyvault secret set \
  --vault-name KVCentral \
  --name DBConnectionString1 \
  --value "Server=sqlserver01;Database=appdb;User Id=admin;Password=SecureP@ssw0rd"
# Create key
az keyvault key create \
  --vault-name KVCentral \
  --name Key1 \
  --protection software \
  --kty RSA \
  --size 2048
# Create certificate
az keyvault certificate create \
  --vault-name KVCentral \
  --name WebApp1 \
  --policy "$(az keyvault certificate get-default-policy --subject 'CN=www.webapp1.local')"

